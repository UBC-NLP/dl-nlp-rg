{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21luw3P3HdAN"
   },
   "source": [
    "# Neural Architecture Search for Efficient Machine Translation Models\n",
    "\n",
    "In this tutorial, we will go over the high-level theory and implementation details of the neural architecture search (NAS) pipeline for identifying efficient machine translation models. The machine translations models are based on the classical encoder-decoder Transformer architectures. These models are trained on machine translation benchmarks from scratch (no pretraining) to convergence. This tutorial borrows the theory and implementation from [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf), which is the state-of-the-art NAS framework to build efficient autoregressive machine translation models.\n",
    "\n",
    "This notebook was created by [Ganesh Jawahar](ganeshjwhr@gmail.com). Contact me for any questions or suggestions.\n",
    "\n",
    "## Prerequisites\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [Transformers](https://arxiv.org/abs/1706.03762)\n",
    "- [Machine Translation](https://github.com/UBC-NLP/itrustai-tutorials/blob/main/machine_translation/Machine_Translation_seq2seq.ipynb)\n",
    "\n",
    "## Problem Setting (General)\n",
    "The goal of neural architecture search is to identify architectures that maximize the accuracy for a user-defined task as much as possible, while satisfying user-defined hardware constraints. Specifically, the input to the neural architecture search is:\n",
    "- **Task:** The NLP task (e.g., autocomplete, machine translation) that the Transformer model should solve.\n",
    "- **Search Space:** Set of candidate Transformer architectures (e.g., varying number of layers, attention heads) that can solve the **task**.\n",
    "- **Constraint:** Constraint on the footprint metric (e.g., $\\leq16$ MB memory or $\\leq200$ ms latency) that the architecture must satisfy.\n",
    "- **Accuracy:** The metric used to quantify the accuracy of the model on the **task**.\n",
    "\n",
    "The method should output the architecture that maximizes the **accuracy** of the model on the **task** from the **search space**, while satisfying the **constraint**. \n",
    "\n",
    "## Problem Setting (This Tutorial)\n",
    "In this tutorial, \n",
    "- **Task:** Machine Translation Task (e.g., WMT 2014 English to German)\n",
    "- **Search Space:**  Set of candidate encoder-decoder Transformer architectures with varying number of decoder layers, embedding size, attention heads (self-attention and cross-attention), feed-forward network (FFN) intermediate size and arbitrary encoder-decoder attention. For arbitrary encoder-decoder attention, -1 means attending to last one encoder layer, 1 means last two encoder layers, 2 means last three encoder layers.\n",
    "\n",
    "| Attributes | Dimensions |\n",
    "| --- | --- |\n",
    "| Encoder-Embedding-Size | [640, 512] |\n",
    "| Decoder-Embedding-Size | [640, 512] |\n",
    "| \\#Encoder-Layers | [6] |\n",
    "| \\#Decoder-Layers | [1, 2, 3, 4, 5, 6] |\n",
    "| Encoder-QKV-Dim | 512 |\n",
    "| Decoder-QKV-Dim | 512 |\n",
    "| \\#Encoder-Self-Att-Heads (Per Layer) | [4, 8] |\n",
    "| \\#Decoder-Self-Att-Heads (Per Layer) | [4, 8] |\n",
    "| \\#Decoder-Cross-Att-Heads (Per Layer) | [4, 8] |\n",
    "| \\#Decoder-Arbitrary-Att (Per Layer) | [-1, 1, 2] |\n",
    "| Encoder-FFN-Intermediate-Size (Per Layer) | [1024, 2048, 3072] |\n",
    "| Decoder-FFN-Intermediate-Size (Per Layer) | [1024, 2048, 3072] |\n",
    "\n",
    "- **Constraint:** $\\leq200$ milliseconds latency (time taken by the model to encode the source sentence and generate the translation sentence in a target hardware (Colab GPU in this example))\n",
    "- **Accuracy:** BLEU score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0opSWSHDUYrn"
   },
   "source": [
    "## Hardware-aware Transformers (Solution)\n",
    "\n",
    "Hardware-aware Transformers (HAT) is a popular NAS framework to solve the problem. HAT has the following stages in the pipeline:\n",
    "1. **Supernet training** - Train a performance estimator that can quickly provide the accuracy of an architecture from the search space\n",
    "2. **Collect hardware latency datasets** - Generate a latency dataset with sample architectures and their corresponding latency measured on target hardware \n",
    "3. **Train latency predictor** - Train a latency estimator on the generated latency dataset\n",
    "4. **Evolutionary search** - Identifies the efficient architecture with accuracy and latency of a candidate architecture from the performance estimator and latency estimator respectively.\n",
    "5. **Train efficient architecture from scratch** - Trains the efficient architecture from scratch to convergence.\n",
    "\n",
    "![HAT block diagram](images/hat.png)\n",
    "\n",
    "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
    "\n",
    "## HAT generated sample architectures\n",
    "Some sample architectures generated by HAT when target hardware is Raspberry Pi (left side) and Titan XP (right side):\n",
    "\n",
    "<img src=\"images/hat_gen_archs.png\" alt=\"HAT generated architectures\" width=\"700\"/>\n",
    "\n",
    "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgpEckM3hxAZ"
   },
   "source": [
    "## HAT Implementation\n",
    "\n",
    "### 0.1 Installation\n",
    "\n",
    "Install HAT by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtGhECPBwLrx",
    "outputId": "b23b4d0a-5a46-4e30-8350-6265e47ccab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'hardware-aware-transformers'...\n",
      "remote: Enumerating objects: 282, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
      "remote: Total 282 (delta 32), reused 23 (delta 23), pack-reused 193\u001b[K\n",
      "Receiving objects: 100% (282/282), 17.09 MiB | 29.97 MiB/s, done.\n",
      "Resolving deltas: 100% (100/100), done.\n",
      "/content/hardware-aware-transformers_1/hardware-aware-transformers\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Obtaining file:///content/hardware-aware-transformers_1/hardware-aware-transformers\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (1.15.1)\n",
      "Requirement already satisfied: fastBPE in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (0.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (1.21.6)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (2022.6.2)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (2.3.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (1.12.1+cu113)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (4.64.1)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (0.29.32)\n",
      "Requirement already satisfied: configargparse in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (1.5.3)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (5.5.0)\n",
      "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (2.5.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from fairseq==0.8.0) (0.0.53)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.8.0) (2.21)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.8.0) (0.8.10)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.8.0) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.8.0) (4.9.1)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->fairseq==0.8.0) (2.6.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->fairseq==0.8.0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->fairseq==0.8.0) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->fairseq==0.8.0) (1.15.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx->fairseq==0.8.0) (3.17.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==0.8.0) (4.1.1)\n",
      "Installing collected packages: fairseq\n",
      "  Running setup.py develop for fairseq\n",
      "Successfully installed fairseq-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mit-han-lab/hardware-aware-transformers.git\n",
    "%cd hardware-aware-transformers\n",
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40Y27shIftkF"
   },
   "source": [
    "### 0.2 Download data\n",
    "\n",
    "Download the preprocessed data for the machine translation task. The syntax is:\n",
    "\n",
    "`bash configs/[task_name]/get_preprocessed.sh`\n",
    "- where `[task_name]` can be `wmt14.en-de`, `wmt14.en-fr`, `wmt19.en-de` and `iwslt14.de-en`.\n",
    "\n",
    "In this tutorial, we will focus on WMT 2014 English to German (`wmt14.en-de`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNuC3wU0HcLH",
    "outputId": "068a27c0-40e4-4489-fa5b-9d86aa4cff63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-10-27 22:22:04--  https://www.dropbox.com/s/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6018:18::a27d:312\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz [following]\n",
      "--2022-10-27 22:22:05--  https://www.dropbox.com/s/raw/axfwl1vawper8yk/wmt16_en_de.preprocessed.tgz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com/cd/0/inline/Bvr4Yw-CtVFociO9exsSEXC9KvoioUmDpmr5u0coQIM_kndwqtbd2Vw-cZGBL0_7qLej8P1bvYO5_nMQN9pe3IN4iQ551Q8-i2WMtlidUydq27muxUapg3bObM6yk1HXrdHG4x3NO_3kfNB-YgL_YUhDrv8J7UmDSSuJBEEWHZj7yA/file# [following]\n",
      "--2022-10-27 22:22:05--  https://ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com/cd/0/inline/Bvr4Yw-CtVFociO9exsSEXC9KvoioUmDpmr5u0coQIM_kndwqtbd2Vw-cZGBL0_7qLej8P1bvYO5_nMQN9pe3IN4iQ551Q8-i2WMtlidUydq27muxUapg3bObM6yk1HXrdHG4x3NO_3kfNB-YgL_YUhDrv8J7UmDSSuJBEEWHZj7yA/file\n",
      "Resolving ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com (ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:6018:15::a27d:30f\n",
      "Connecting to ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com (ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BvoTDuTa27BLPrtWd5aqUwhlc5X0-gwljSaUwL7GvJJ8EB5z63cLxsNbG7s_JWM_cS-6OetjjQDZeTuoiJ7fEeiwROaMH6f_Yrz0vLpnc5gtjJlNwEjADUWDxJRMcPlHpZs3BNFheoCN-KPwDxkVzs7yoMN6Q9E7aVRYa5q836HeP0fP2nFCtUl9ZMTlnWKRQ7kCB2B6Ie_iMdVtGkacykCBeGwIqiwDID00-M0IVdsiLJ3IiFkXFZj5yikxuqU0HpFT5fWRcqbzaubtsawciOQZZnmJhBGDyd_iAPIO86YPWpsONtSKPMKACIgjIN0i09mH9uxLYbJnpURDDbZWO4ljhgadaWaO4PmDQDzZvs6wtCG3NachKYFvqKfbbRTR4BWl633tqUxDeVKNmDUrD8oUdWd0YCYmBu3uwZJP8-UiFA/file [following]\n",
      "--2022-10-27 22:22:05--  https://ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com/cd/0/inline2/BvoTDuTa27BLPrtWd5aqUwhlc5X0-gwljSaUwL7GvJJ8EB5z63cLxsNbG7s_JWM_cS-6OetjjQDZeTuoiJ7fEeiwROaMH6f_Yrz0vLpnc5gtjJlNwEjADUWDxJRMcPlHpZs3BNFheoCN-KPwDxkVzs7yoMN6Q9E7aVRYa5q836HeP0fP2nFCtUl9ZMTlnWKRQ7kCB2B6Ie_iMdVtGkacykCBeGwIqiwDID00-M0IVdsiLJ3IiFkXFZj5yikxuqU0HpFT5fWRcqbzaubtsawciOQZZnmJhBGDyd_iAPIO86YPWpsONtSKPMKACIgjIN0i09mH9uxLYbJnpURDDbZWO4ljhgadaWaO4PmDQDzZvs6wtCG3NachKYFvqKfbbRTR4BWl633tqUxDeVKNmDUrD8oUdWd0YCYmBu3uwZJP8-UiFA/file\n",
      "Reusing existing connection to ucf9f84141e9e6fc43a10f7befc9.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 477487184 (455M) [application/x-gtar]\n",
      "Saving to: ‘data/binary/wmt16_en_de/wmt16_en_de.preprocessed.tgz’\n",
      "\n",
      "data/binary/wmt16_e 100%[===================>] 455.37M  84.3MB/s    in 5.5s    \n",
      "\n",
      "2022-10-27 22:22:11 (83.5 MB/s) - ‘data/binary/wmt16_en_de/wmt16_en_de.preprocessed.tgz’ saved [477487184/477487184]\n",
      "\n",
      "./\n",
      "./train.en-de.en.idx\n",
      "./valid.en-de.en.bin\n",
      "./valid.en-de.en.idx\n",
      "./test.en-de.en.bin\n",
      "./test.en-de.en.idx\n",
      "./train.en-de.de.bin\n",
      "./train.en-de.de.idx\n",
      "./valid.en-de.de.bin\n",
      "./valid.en-de.de.idx\n",
      "./test.en-de.de.bin\n",
      "./test.en-de.de.idx\n",
      "./dict.en.txt\n",
      "./dict.de.txt\n",
      "./train.en-de.en.bin\n"
     ]
    }
   ],
   "source": [
    "!bash configs/wmt14.en-de/get_preprocessed.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbTWn0m6i1kB"
   },
   "source": [
    "### 0.3 Inspect the search space\n",
    "\n",
    "Look at the search space config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1DYh_S1gh1CY",
    "outputId": "8e29d3fb-2c14-4952-eb3d-8039febc4d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# model\n",
      "arch: transformersuper_wmt_en_de\n",
      "share-all-embeddings: True\n",
      "max-tokens: 4096\n",
      "data: data/binary/wmt16_en_de\n",
      "\n",
      "# training settings\n",
      "optimizer: adam\n",
      "adam-betas: (0.9, 0.98)\n",
      "clip-norm: 0.0\n",
      "weight-decay: 0.0\n",
      "dropout: 0.3\n",
      "attention-dropout: 0.1\n",
      "criterion: label_smoothed_cross_entropy\n",
      "label-smoothing: 0.1\n",
      "\n",
      "ddp-backend: no_c10d\n",
      "fp16: True\n",
      "\n",
      "# warmup from warmup-init-lr to max-lr (warmup-updates steps); then cosine anneal to lr (max-update - warmup-updates steps)\n",
      "update-freq: 16\n",
      "max-update: 40000\n",
      "warmup-updates: 10000\n",
      "lr-scheduler: cosine\n",
      "warmup-init-lr: 1e-7\n",
      "max-lr: 0.001\n",
      "lr: 1e-7\n",
      "lr-shrink: 1\n",
      "\n",
      "# logging\n",
      "keep-last-epochs: 20\n",
      "save-interval: 10\n",
      "validate-interval: 10\n",
      "\n",
      "# SuperTransformer configs\n",
      "encoder-embed-dim: 640\n",
      "decoder-embed-dim: 640\n",
      "\n",
      "encoder-ffn-embed-dim: 3072\n",
      "decoder-ffn-embed-dim: 3072\n",
      "\n",
      "encoder-layers: 6\n",
      "decoder-layers: 6\n",
      "\n",
      "encoder-attention-heads: 8\n",
      "decoder-attention-heads: 8\n",
      "\n",
      "qkv-dim: 512\n",
      "\n",
      "# SubTransformers search space\n",
      "encoder-embed-choice: [640, 512]\n",
      "decoder-embed-choice: [640, 512]\n",
      "\n",
      "encoder-ffn-embed-dim-choice: [3072, 2048, 1024]\n",
      "decoder-ffn-embed-dim-choice: [3072, 2048, 1024]\n",
      "\n",
      "encoder-layer-num-choice: [6]\n",
      "decoder-layer-num-choice: [6, 5, 4, 3, 2, 1]\n",
      "\n",
      "encoder-self-attention-heads-choice: [8, 4]\n",
      "decoder-self-attention-heads-choice: [8, 4]\n",
      "decoder-ende-attention-heads-choice: [8, 4]\n",
      "\n",
      "# for arbitrary encoder decoder attention. -1 means attending to last one encoder layer\n",
      "# 1 means last two encoder layers, 2 means last three encoder layers\n",
      "decoder-arbitrary-ende-attn-choice: [-1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "!cat configs/wmt14.en-de/supertransformer/space0.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNEe-krdjT55"
   },
   "source": [
    "`# SubTransformers search space` marks the search space for NAS, that defines the possible values taken by each Transformer hyperparameter.\n",
    "\n",
    "### 1. Supernet training\n",
    "\n",
    "A typical challenge in the NAS framework is to develop a performance estimator that can efficiently compute the accuracy of a candidate architecture. The naive approach of training candidate architectures from scratch to convergence and then evaluating on the validation set is prohibitively expensive given the large search space for all possible candidate architectures.\n",
    "\n",
    "HAT's performance estimator is based on weight-sharing via a Supernet. The supernet is the \n",
    "largest model in the search space (marked by `# SuperTransformer configs` in the previous config file).\n",
    "\n",
    "\n",
    "The Supernet is trained with the following steps:  \n",
    "1. sample a candidate architecture randomly from the search space\n",
    "2. train the sampled architecture by extracting the common portion of weights (subnet extraction) from different layers in the Supernet (i.e., by weight sharing) for one training step on the task\n",
    "3. repeat steps 1 and 2 until the training budget is exhausted. \n",
    "\n",
    "Once the Supernet training is complete, we can obtain a quick accuracy estimate for a candidate architecture (i.e. subnetwork) by extracting its shared weights from the Supernet and evaluating on the validation set.\n",
    "\n",
    "Let us understand how subnet extraction work via ``nn.Linear`` layer. As shown below, assume a linear layer in Supernet has 640 input features and 1024 output features ($1024\\times 640$). Say, the same linear layer in subnet has only 512 input features and 768 output features ($768\\times 512$). The linear layer weights for the subnet can be constructed by extracting the first 512 columns and first 768 rows from the corresponding weights of the supernet.\n",
    "\n",
    "<img src=\"images/linear_weight_sharing.png\" alt=\"HAT generated architectures\" width=\"300\"/>\n",
    "\n",
    "(Picture courtesy: [Hardware-Aware Transformers](https://arxiv.org/pdf/2005.14187.pdf))\n",
    "\n",
    "\n",
    "Here's a sample implementation of ``nn.LinearSuper`` that generalizes ``nn.Linear``:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TL0xlhKRjQsf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearSuper(nn.Linear):\n",
    "    def __init__(self, super_in_dim, super_out_dim, bias=True, uniform_=None, non_linear='linear'):\n",
    "        super().__init__(super_in_dim, super_out_dim, bias=bias)\n",
    "\n",
    "        # super_in_dim and super_out_dim indicate the largest network!\n",
    "        self.super_in_dim = super_in_dim\n",
    "        self.super_out_dim = super_out_dim\n",
    "\n",
    "        # input_dim and output_dim indicate the current sampled size\n",
    "        self.sample_in_dim = None\n",
    "        self.sample_out_dim = None\n",
    "\n",
    "        self.samples = {}\n",
    "\n",
    "        self._reset_parameters(bias, uniform_, non_linear)\n",
    "        self.profiling = False\n",
    "\n",
    "    def profile(self, mode=True):\n",
    "        self.profiling = mode\n",
    "\n",
    "    def sample_parameters(self, resample=False):\n",
    "        if self.profiling or resample:\n",
    "            return self._sample_parameters()\n",
    "        return self.samples\n",
    "\n",
    "    def _reset_parameters(self, bias, uniform_, non_linear):\n",
    "        nn.init.xavier_uniform_(self.weight) if uniform_ is None else uniform_(\n",
    "            self.weight, non_linear=non_linear)\n",
    "        if bias:\n",
    "            nn.init.constant_(self.bias, 0.)\n",
    "\n",
    "    def set_sample_config(self, sample_in_dim, sample_out_dim):\n",
    "        self.sample_in_dim = sample_in_dim\n",
    "        self.sample_out_dim = sample_out_dim\n",
    "\n",
    "        self._sample_parameters()\n",
    "\n",
    "    def _sample_parameters(self):\n",
    "        self.samples['weight'] = sample_weight(self.weight, self.sample_in_dim, self.sample_out_dim)\n",
    "        self.samples['bias'] = self.bias\n",
    "        if self.bias is not None:\n",
    "            self.samples['bias'] = sample_bias(self.bias, self.sample_out_dim)\n",
    "        return self.samples\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sample_parameters()\n",
    "        return F.linear(x, self.samples['weight'], self.samples['bias'])\n",
    "\n",
    "    def calc_sampled_param_num(self):\n",
    "        assert 'weight' in self.samples.keys()\n",
    "        weight_numel = self.samples['weight'].numel()\n",
    "\n",
    "        if self.samples['bias'] is not None:\n",
    "            bias_numel = self.samples['bias'].numel()\n",
    "        else:\n",
    "            bias_numel = 0\n",
    "\n",
    "        return weight_numel + bias_numel\n",
    "\n",
    "# weight extraction for subnet\n",
    "def sample_weight(weight, sample_in_dim, sample_out_dim):\n",
    "    sample_weight = weight[:, :sample_in_dim] # extract first `sample_in_dim` columns\n",
    "    sample_weight = sample_weight[:sample_out_dim, :] # extract first `sample_out_dim` columns\n",
    "\n",
    "    return sample_weight\n",
    "\n",
    "# bias extraction for subnet\n",
    "def sample_bias(bias, sample_out_dim):\n",
    "    sample_bias = bias[:sample_out_dim] # extract first `sample_out_dim` numbers\n",
    "\n",
    "    return sample_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX8HeBsYvie4"
   },
   "source": [
    "Let us construct the linear layer for Supernet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yePG_jyWuwDr",
    "outputId": "2aa1b195-15e1-4b9f-c5c5-1bfc74434271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supernet: weight shape =  torch.Size([1024, 640])\n",
      "Supernet: bias shape =  torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "linearlayer_supernet = LinearSuper(super_in_dim=640, super_out_dim=1024)\n",
    "# print the shape of weight matrix\n",
    "print(\"Supernet: weight shape = \", linearlayer_supernet.weight.shape)\n",
    "# print the shape of bias matrix\n",
    "print(\"Supernet: bias shape = \", linearlayer_supernet.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqAC3fdjwY88"
   },
   "source": [
    "To extract the subnet weights, we use `set_sample_config()` function specifying the input and the output features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehOQ_aZIwp5K",
    "outputId": "7e4634bd-4e46-40ba-b899-a1772f8c7eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet: weight shape =  torch.Size([768, 512])\n",
      "Subnet: bias shape =  torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "linearlayer_supernet.set_sample_config(sample_in_dim=512, sample_out_dim=768)\n",
    "# print the shape of weight matrix\n",
    "print(\"Subnet: weight shape = \", linearlayer_supernet.samples['weight'].shape)\n",
    "# print the shape of bias matrix\n",
    "print(\"Subnet: bias shape = \", linearlayer_supernet.samples['bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29_b-HMYxOln"
   },
   "source": [
    "Refer to `fairseq/modules` to see subnet extraction implementation for other Trasnformer layers, e.g., `multihead_attention_super` (self-attention), `embedding_super` (embedding layer).\n",
    "\n",
    "Let us train a supernet for few steps now (change `max-tokens`, `max-update`, `update-freq` for full training). Before running the following command, change the line 198 of `fairseq/modules/multihead_attention_super.py` from `q *= self.scaling` to `q = q *self.scaling` (to avoid error: `RuntimeError: Output 0 of SplitBackward0 is a view and is being modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpIr3MUHyIus",
    "outputId": "c29ffb9d-0fa8-4ec1-ff79-43a1449d912f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Configs: Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=0.0, configs='configs/wmt14.en-de/supertransformer/space0.yml', cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, latcpu=False, latgpu=False, latiter=300, latsilent=False, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=5, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_flops=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='baseline/supernet', save_interval=10, save_interval_updates=5, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, sub_configs=None, t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='baseline/supernet/tensorboard', threshold_loss_scale=None, train_subset='train', train_subtransformer=False, unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=10, validate_subtransformer=False, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=10000, weight_decay=0.0)\n",
      "| [en] dictionary: 32768 types\n",
      "| [de] dictionary: 32768 types\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
      "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
      "| Fallback to xavier initializer\n",
      "| Model: transformersuper_wmt_en_de \n",
      "| Criterion: LabelSmoothedCrossEntropyCriterion\n",
      " \n",
      "\n",
      "\t\tWARNING!!! Training SuperTransformer\n",
      "\n",
      "\n",
      "| SuperTransformer Arch: TransformerSuperModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "| SuperTransofmer model size (without embedding weights): 70900992\n",
      "| Embedding layer size: 20971520 \n",
      "\n",
      "| Training on 1 GPUs\n",
      "| Max tokens per GPU = 4096 and max sentences per GPU = None \n",
      "\n",
      "| no existing checkpoint found baseline/supernet/checkpoint_last.pt\n",
      "| loading train data for epoch 0\n",
      "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.en\n",
      "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.de\n",
      "| data/binary/wmt16_en_de train en-de 4500966 examples\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| epoch 001:   0% 0/2376 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| epoch 001:   0% 1/2376 [00:03<2:35:35,  3.93s/it]| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| epoch 001:   0% 2/2376 [00:06<1:54:38,  2.90s/it]| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 001:   0% 3/2376 [00:08<1:39:40,  2.52s/it]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 001:   0% 4/2376 [00:10<1:32:44,  2.35s/it]/content/hardware-aware-transformers/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "| epoch 001:   0% 8/2376 [00:18<1:23:50,  2.12s/it, loss=15.691, wps=3420, num_updates=4, lr=4.9996e-07]\n",
      "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:17,  1.67it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  10% 3/30 [00:00<00:05,  5.17it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  17% 5/30 [00:00<00:03,  8.22it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  23% 7/30 [00:00<00:02, 10.58it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  30% 9/30 [00:01<00:01, 12.30it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  37% 11/30 [00:01<00:01, 13.96it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  43% 13/30 [00:01<00:01, 15.27it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  50% 15/30 [00:01<00:00, 16.29it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  57% 17/30 [00:01<00:00, 16.52it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  63% 19/30 [00:01<00:00, 16.96it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  70% 21/30 [00:01<00:00, 17.48it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  77% 23/30 [00:01<00:00, 17.69it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  83% 25/30 [00:01<00:00, 17.27it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  90% 27/30 [00:02<00:00, 16.95it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  97% 29/30 [00:02<00:00, 16.92it/s]\u001b[A\n",
      "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 15.627 | nll_loss 15.617 | ppl 50240.75 | num_updates 5 | largest_arbitrary1_loss 15.627 | largest_arbitrary1_nll_loss 15.617\n",
      "\n",
      "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:15,  1.93it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  13% 4/30 [00:00<00:03,  7.59it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  27% 8/30 [00:00<00:01, 14.69it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  43% 13/30 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  60% 18/30 [00:00<00:00, 27.73it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  73% 22/30 [00:01<00:00, 30.76it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  87% 26/30 [00:01<00:00, 32.52it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset: 100% 30/30 [00:01<00:00, 34.18it/s]\u001b[A\n",
      "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 20.431 | nll_loss 20.423 | ppl 1406188.68 | num_updates 5 | smallest_arbitrary1_loss 20.431 | smallest_arbitrary1_nll_loss 20.423\n",
      "| saved checkpoint baseline/supernet/checkpoint_1_5.pt (epoch 1 @ 5 updates) (writing took 11.575386762619019 seconds)\n",
      "| epoch 001 | loss 15.911 | nll_loss 15.905 | ppl 61368.88 | wps 3415 | ups 0 | wpb 59097.200 | bsz 1984.000 | num_updates 5 | lr 5.9995e-07 | gnorm 4.843 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 87 | train_wall 18\n",
      "| Done training in 67.7 seconds\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p baseline/supernet # stores supernet checkpoint\n",
    "!python -B train.py --configs=configs/wmt14.en-de/supertransformer/space0.yml --save-dir baseline/supernet --no-epoch-checkpoints --max-update 5 --save-interval-updates 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iWJQIsg3SEO"
   },
   "source": [
    "The best supernet checkpoint can be accessed at `baseline/supernet/checkpoint_best.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwOVTX9z1owH"
   },
   "source": [
    "### 2. Collect hardware latency datasets\n",
    "\n",
    "In the next step, we will generate the hardware latency datasets, which will be subsequently used to train a latency prediction model. This step will sample architectures from the search space and measure the latency of the architecture on the target hardware.\n",
    "\n",
    "Create a small dataset by running the following command (remove `--lat-dataset-size 25` to generate the full dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZVqgz4K2KeT",
    "outputId": "dded1cc0-dde7-4917-e9f6-3df18f625a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='relu', adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.0, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=25, configs='configs/wmt14.en-de/latency_dataset/gpu_titanxp.yml', cpu=False, criterion='cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4, 2], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024, 512], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4, 2], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.1, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024, 512], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4, 2], find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=-1, lat_dataset_path='baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv', lat_dataset_size=25, latcpu=False, latgpu=True, latiter=20, latsilent=True, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='nag', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir=None, save_interval=1, save_interval_updates=0, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, vocab_original_scaling=False, warmup_updates=0, weight_decay=0.0)\n",
      "| [en] dictionary: 32768 types\n",
      "| [de] dictionary: 32768 types\n",
      "| Fallback to xavier initializer\n",
      "TransformerSuperModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Measuring model latency on GPU for dataset generation...\n",
      "0\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 185.68it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.25397002696991 ms; \t Std: 0.12618696079534436 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  6.83it/s]\n",
      "[139.238525390625, 141.15020751953125, 142.0160369873047, 142.0248260498047, 142.37954711914062, 143.28192138671875, 143.8572235107422, 144.3555450439453, 145.81922912597656, 146.47203063964844, 147.14285278320312, 148.43267822265625, 150.48089599609375, 151.4660186767578, 152.6559295654297, 153.17955017089844]\n",
      "Decoder latency for dataset generation: Mean: 145.87206363677979 ms; \t Std: 4.196159854532478 ms\n",
      "1\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 187.93it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.262547969818115 ms; \t Std: 0.11428225048039717 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  6.61it/s]\n",
      "[144.09727478027344, 144.1128692626953, 145.61141967773438, 146.01580810546875, 146.06597900390625, 147.7672576904297, 148.43206787109375, 148.85289001464844, 149.48345947265625, 149.9811553955078, 151.72402954101562, 152.98048400878906, 153.34182739257812, 154.03213500976562, 155.59756469726562, 156.30560302734375]\n",
      "Decoder latency for dataset generation: Mean: 149.65011405944824 ms; \t Std: 3.8531895874436066 ms\n",
      "2\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 177.67it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.36736398935318 ms; \t Std: 0.2045495893060776 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:04<00:00,  4.38it/s]\n",
      "[219.78775024414062, 220.17269897460938, 220.34230041503906, 220.7403564453125, 222.29388427734375, 222.4003448486328, 222.61558532714844, 223.02316284179688, 223.40127563476562, 224.32266235351562, 224.65501403808594, 225.71621704101562, 227.5082550048828, 235.84713745117188, 236.45901489257812, 236.8431396484375]\n",
      "Decoder latency for dataset generation: Mean: 225.38304996490479 ms; \t Std: 5.6502403918365784 ms\n",
      "3\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 172.95it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.525900036096573 ms; \t Std: 0.29948850256262943 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  8.43it/s]\n",
      "[113.35155487060547, 113.63394927978516, 114.39302062988281, 114.5712661743164, 114.8109130859375, 115.0601577758789, 115.11763000488281, 115.51538848876953, 116.60546875, 116.99158477783203, 117.07392120361328, 117.2142105102539, 117.51516723632812, 117.81068420410156, 118.16307067871094, 121.66009521484375]\n",
      "Decoder latency for dataset generation: Mean: 116.21800518035889 ms; \t Std: 2.0318351476168277 ms\n",
      "4\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 164.24it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.912278026342392 ms; \t Std: 0.2585194912244389 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:04<00:00,  4.29it/s]\n",
      "[221.62896728515625, 222.60733032226562, 223.1227569580078, 225.31475830078125, 225.97427368164062, 226.1378173828125, 227.378662109375, 228.35548400878906, 229.1875457763672, 230.92247009277344, 231.53053283691406, 232.1717071533203, 233.7325439453125, 239.47567749023438, 243.54185485839844, 246.54336547851562]\n",
      "Decoder latency for dataset generation: Mean: 230.4766092300415 ms; \t Std: 7.072804358742522 ms\n",
      "5\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 168.33it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.563307970762253 ms; \t Std: 0.22549846025700626 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:04<00:00,  4.72it/s]\n",
      "[190.93724060058594, 190.9892120361328, 194.19020080566406, 195.90147399902344, 195.94883728027344, 198.8732452392578, 199.0411834716797, 200.8842315673828, 201.18077087402344, 201.59170532226562, 202.86444091796875, 204.9147186279297, 205.87318420410156, 222.85926818847656, 234.41868591308594, 245.46511840820312]\n",
      "Decoder latency for dataset generation: Mean: 205.37084484100342 ms; \t Std: 15.034912047490737 ms\n",
      "6\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 164.29it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.938933998346329 ms; \t Std: 0.6785129465879717 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  5.22it/s]\n",
      "[182.80038452148438, 183.80438232421875, 184.63363647460938, 185.13580322265625, 186.02560424804688, 186.41043090820312, 188.67408752441406, 188.89010620117188, 189.79696655273438, 189.95376586914062, 190.91830444335938, 191.93209838867188, 191.9815673828125, 192.24252319335938, 193.11436462402344, 201.56182861328125]\n",
      "Decoder latency for dataset generation: Mean: 189.24224090576172 ms; \t Std: 4.486871672897566 ms\n",
      "7\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 167.93it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.714330017566681 ms; \t Std: 0.30839007987026945 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 21.28it/s]\n",
      "[44.226558685302734, 44.36377716064453, 44.61606216430664, 44.78908920288086, 44.907264709472656, 45.0437126159668, 45.314048767089844, 45.57606506347656, 45.7441291809082, 46.28611373901367, 46.631614685058594, 46.8317756652832, 47.16569519042969, 48.1710090637207, 48.23302459716797, 48.312416076660156]\n",
      "Decoder latency for dataset generation: Mean: 46.013272285461426 ms; \t Std: 1.3602345161332023 ms\n",
      "8\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 171.19it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.595581948757172 ms; \t Std: 0.3044933678917262 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  8.61it/s]\n",
      "[111.08819580078125, 111.30448150634766, 111.87171173095703, 112.20384216308594, 112.24153900146484, 112.7303695678711, 113.30364990234375, 114.2476806640625, 115.84307098388672, 115.87200164794922, 116.04259490966797, 116.487548828125, 119.07305908203125, 119.88553619384766, 120.57465362548828, 121.98889923095703]\n",
      "Decoder latency for dataset generation: Mean: 115.2974271774292 ms; \t Std: 3.414152316886711 ms\n",
      "9\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 173.86it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.513493984937668 ms; \t Std: 0.2165088518233732 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 20.58it/s]\n",
      "[45.85295867919922, 45.97785568237305, 46.134464263916016, 46.1844482421875, 46.26451110839844, 47.122432708740234, 48.04198455810547, 48.379905700683594, 48.53843307495117, 48.65865707397461, 49.44803237915039, 49.48793411254883, 49.66572952270508, 49.872161865234375, 50.093055725097656, 51.07583999633789]\n",
      "Decoder latency for dataset generation: Mean: 48.17490029335022 ms; \t Std: 1.659627311029607 ms\n",
      "10\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 175.35it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.621161967515945 ms; \t Std: 0.1821805481177297 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:05<00:00,  3.84it/s]\n",
      "[220.96278381347656, 221.47279357910156, 222.83045959472656, 225.0650634765625, 229.84909057617188, 230.1071319580078, 233.39427185058594, 234.46937561035156, 248.7070770263672, 274.1166076660156, 287.76446533203125, 289.4985046386719, 289.7574768066406, 292.7119140625, 296.9456787109375, 314.743896484375]\n",
      "Decoder latency for dataset generation: Mean: 257.0247869491577 ms; \t Std: 32.51564309241865 ms\n",
      "11\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 173.82it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.4950200617313385 ms; \t Std: 0.11665084068045917 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:04<00:00,  4.48it/s]\n",
      "[214.8679656982422, 216.3323211669922, 217.53814697265625, 218.91091918945312, 219.09913635253906, 219.44525146484375, 220.39305114746094, 220.64755249023438, 220.9764404296875, 222.63340759277344, 224.45872497558594, 227.30313110351562, 227.36312866210938, 228.0400390625, 229.34326171875, 231.93402099609375]\n",
      "Decoder latency for dataset generation: Mean: 222.45540618896484 ms; \t Std: 4.885851867634457 ms\n",
      "12\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 185.20it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.358407914638519 ms; \t Std: 0.1375578146744895 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  8.62it/s]\n",
      "[110.62710571289062, 110.73638153076172, 110.7988510131836, 112.52428436279297, 112.560546875, 113.02738952636719, 113.43417358398438, 113.89695739746094, 115.9352035522461, 116.34457397460938, 117.80976104736328, 117.92787170410156, 119.09939575195312, 119.39635467529297, 121.42591857910156, 121.51602935791016]\n",
      "Decoder latency for dataset generation: Mean: 115.44129991531372 ms; \t Std: 3.62722259116528 ms\n",
      "13\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 174.91it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.4797380566596985 ms; \t Std: 0.2613676501933075 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  6.56it/s]\n",
      "[145.86093139648438, 147.21353149414062, 147.3314208984375, 148.39193725585938, 148.7638702392578, 149.18508911132812, 150.41127014160156, 151.26051330566406, 152.5207061767578, 152.93667602539062, 153.0914306640625, 154.1116485595703, 154.27789306640625, 154.2920379638672, 154.57626342773438, 160.37887573242188]\n",
      "Decoder latency for dataset generation: Mean: 151.53775596618652 ms; \t Std: 3.601599863820905 ms\n",
      "14\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 170.78it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.633345991373062 ms; \t Std: 0.34689148748372506 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  5.04it/s]\n",
      "[182.14498901367188, 183.01113891601562, 184.1131134033203, 186.9905548095703, 187.3059844970703, 187.525146484375, 188.8155517578125, 189.92672729492188, 193.12025451660156, 193.33689880371094, 196.93344116210938, 197.557861328125, 199.4035186767578, 202.3416290283203, 204.2442169189453, 209.6537628173828]\n",
      "Decoder latency for dataset generation: Mean: 192.90154933929443 ms; \t Std: 7.867335536339601 ms\n",
      "15\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 163.30it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.855192005634308 ms; \t Std: 0.8524735946773079 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:01<00:00, 11.86it/s]\n",
      "[78.67391967773438, 78.83631896972656, 78.94425964355469, 79.52003479003906, 80.15872192382812, 81.13011169433594, 81.99779510498047, 83.34950256347656, 84.26156616210938, 84.7825927734375, 85.42530822753906, 85.73567962646484, 85.88700866699219, 86.80550384521484, 86.86438751220703, 92.32240295410156]\n",
      "Decoder latency for dataset generation: Mean: 83.41844463348389 ms; \t Std: 3.685252614386576 ms\n",
      "16\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 170.80it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.618268013000488 ms; \t Std: 0.19546831005565546 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  5.99it/s]\n",
      "[153.46841430664062, 154.1447296142578, 154.33523559570312, 156.14157104492188, 156.67344665527344, 157.2515869140625, 162.45960998535156, 163.43008422851562, 163.66592407226562, 166.8348846435547, 169.69903564453125, 170.52671813964844, 172.18368530273438, 173.25628662109375, 177.26364135742188, 186.57833862304688]\n",
      "Decoder latency for dataset generation: Mean: 164.86957454681396 ms; \t Std: 9.268913357367857 ms\n",
      "17\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 172.89it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.709636002779007 ms; \t Std: 0.18946621679012204 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  5.19it/s]\n",
      "[185.45663452148438, 186.7610626220703, 187.74826049804688, 188.0513916015625, 190.17015075683594, 190.2809295654297, 190.610107421875, 191.16441345214844, 191.4654998779297, 191.78445434570312, 192.2852783203125, 195.4935302734375, 195.7960968017578, 199.22738647460938, 199.29493713378906, 201.27481079101562]\n",
      "Decoder latency for dataset generation: Mean: 192.3040590286255 ms; \t Std: 4.540699272090148 ms\n",
      "18\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 170.89it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.588095963001251 ms; \t Std: 0.3948525881141642 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  6.46it/s]\n",
      "[147.1464385986328, 147.6037139892578, 147.74755859375, 148.51683044433594, 151.64210510253906, 152.0945281982422, 152.35816955566406, 153.316162109375, 153.34591674804688, 156.17190551757812, 156.8753662109375, 157.9884490966797, 157.99404907226562, 159.0087432861328, 159.3159942626953, 164.3540496826172]\n",
      "Decoder latency for dataset generation: Mean: 154.09249877929688 ms; \t Std: 4.841779885768009 ms\n",
      "19\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 180.99it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.478619962930679 ms; \t Std: 0.10436618381385009 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:01<00:00, 11.95it/s]\n",
      "[77.58243560791016, 78.03228759765625, 78.62640380859375, 79.12188720703125, 79.54573059082031, 79.85536193847656, 80.3780517578125, 81.39775848388672, 83.75091552734375, 83.99075317382812, 84.00310516357422, 84.54345703125, 85.64310455322266, 86.81884765625, 87.08041381835938, 89.26617431640625]\n",
      "Decoder latency for dataset generation: Mean: 82.47729301452637 ms; \t Std: 3.513416159463502 ms\n",
      "20\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 160.82it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.977419972419739 ms; \t Std: 1.0019118141954126 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 20.03it/s]\n",
      "[46.104576110839844, 46.21104049682617, 46.21993637084961, 46.3240966796875, 46.39622497558594, 46.7147216796875, 46.77065658569336, 47.16352081298828, 47.28227233886719, 47.41059112548828, 47.8422737121582, 48.372222900390625, 49.09849548339844, 50.3996467590332, 50.61008071899414, 51.255775451660156]\n",
      "Decoder latency for dataset generation: Mean: 47.76100826263428 ms; \t Std: 1.652554578543356 ms\n",
      "21\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 186.34it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.319584012031555 ms; \t Std: 0.11188657687223046 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:01<00:00, 12.28it/s]\n",
      "[77.9487075805664, 78.18109130859375, 78.2176284790039, 78.27500915527344, 79.54476928710938, 80.08089447021484, 80.33452606201172, 80.47411346435547, 80.81017303466797, 81.17024230957031, 81.5567398071289, 82.05644989013672, 83.81619262695312, 84.21167755126953, 85.22822570800781, 85.4097900390625]\n",
      "Decoder latency for dataset generation: Mean: 81.08226442337036 ms; \t Std: 2.4116695385072746 ms\n",
      "22\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 165.75it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.748246014118195 ms; \t Std: 0.7107898604589615 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:03<00:00,  5.14it/s]\n",
      "[185.24620056152344, 185.33721923828125, 187.79478454589844, 188.4812469482422, 188.53219604492188, 189.1082305908203, 190.84902954101562, 192.4958038330078, 192.58505249023438, 193.231201171875, 198.1814727783203, 199.10665893554688, 199.51890563964844, 200.2698211669922, 200.4029083251953, 200.53773498535156]\n",
      "Decoder latency for dataset generation: Mean: 193.2299041748047 ms; \t Std: 5.459091360847616 ms\n",
      "23\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 168.52it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.623448044061661 ms; \t Std: 0.26515534175637073 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  8.63it/s]\n",
      "[108.88130950927734, 109.31199645996094, 111.3884506225586, 112.35740661621094, 112.41500854492188, 112.66636657714844, 113.32176208496094, 113.96300506591797, 114.5728988647461, 114.6081314086914, 115.98220825195312, 117.2266845703125, 119.31062316894531, 120.4576644897461, 122.12313842773438, 123.74342346191406]\n",
      "Decoder latency for dataset generation: Mean: 115.1456298828125 ms; \t Std: 4.237568925212577 ms\n",
      "24\n",
      "Measuring encoder for dataset generation...\n",
      "100% 20/20 [00:00<00:00, 183.06it/s]\n",
      "Encoder latency for dataset generation: Mean: 5.348716080188751 ms; \t Std: 0.23854407742423006 ms\n",
      "Measuring decoder for dataset generation...\n",
      "100% 20/20 [00:02<00:00,  8.57it/s]\n",
      "[111.48377227783203, 112.03033447265625, 112.44588470458984, 112.87747192382812, 113.7192611694336, 115.09465789794922, 115.3135986328125, 115.53314971923828, 115.68390655517578, 117.22547149658203, 117.3012466430664, 117.45702362060547, 118.07263946533203, 118.32653045654297, 119.98294067382812, 121.81094360351562]\n",
      "Decoder latency for dataset generation: Mean: 115.89742708206177 ms; \t Std: 2.8486396036360557 ms\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p baseline/genlatdata # to save latency dataset\n",
    "!CUDA_VISIBLE_DEVICES=0 python latency_dataset.py --configs=configs/wmt14.en-de/latency_dataset/gpu_titanxp.yml  --lat-dataset-path baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv --lat-dataset-size 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO5GuMtt3LZO"
   },
   "source": [
    "The latency dataset can be accessed at `baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI_57HA_3iBf"
   },
   "source": [
    "### 3. Train latency predictor \n",
    "\n",
    "After generating the latency dataset, we can train a latency predictor. HAT's predictor is based on a simple 2-layer MLP based regressor.\n",
    "\n",
    "Run the following command to train latency predictor (remove `--bsz 2` for full run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep2AVvJw3hB4",
    "outputId": "158e0b21-d3fe-4103-e720-f9afd4211828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(bsz=2, ckpt_path='baseline/latpred/wmt14.en-de_gpu_titanxp.pt', configs='configs/wmt14.en-de/latency_predictor/gpu_titanxp.yml', dataset_path=None, feature_dim=10, feature_norm=[640.0, 6.0, 2048.0, 6.0, 640.0, 6.0, 2048.0, 6.0, 6.0, 2.0], hidden_dim=400, hidden_layer_num=3, lat_dataset_path='baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv', lat_norm=200.0, lr=1e-05, train_steps=5000)\n",
      "latency_predictor.py:75: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  sample_x_tensor = torch.Tensor(sample_x)\n",
      "Validation loss at 0 steps: 0.6297323107719421\n",
      "Validation loss at 100 steps: 0.3690260350704193\n",
      "Validation loss at 200 steps: 0.14750224351882935\n",
      "Validation loss at 300 steps: 0.10700798034667969\n",
      "Validation loss at 400 steps: 0.09164999425411224\n",
      "Validation loss at 500 steps: 0.07081913948059082\n",
      "Validation loss at 600 steps: 0.05648641288280487\n",
      "Validation loss at 700 steps: 0.043196901679039\n",
      "Validation loss at 800 steps: 0.03159669041633606\n",
      "Validation loss at 900 steps: 0.02095351181924343\n",
      "Validation loss at 1000 steps: 0.013059407472610474\n",
      "Validation loss at 1100 steps: 0.00757997389882803\n",
      "Validation loss at 1200 steps: 0.004985300824046135\n",
      "Validation loss at 1300 steps: 0.0038831671699881554\n",
      "Validation loss at 1400 steps: 0.0018117158906534314\n",
      "Validation loss at 1500 steps: 0.0020595646928995848\n",
      "Validation loss at 1600 steps: 0.0015151894185692072\n",
      "Validation loss at 1700 steps: 0.002054190030321479\n",
      "Validation loss at 1800 steps: 0.0015302211977541447\n",
      "Validation loss at 1900 steps: 0.0015811705961823463\n",
      "Validation loss at 2000 steps: 0.0006182038923725486\n",
      "Validation loss at 2100 steps: 0.0015657315962016582\n",
      "Validation loss at 2200 steps: 0.0015474397223442793\n",
      "Validation loss at 2300 steps: 0.0006345061119645834\n",
      "Validation loss at 2400 steps: 0.002048045163974166\n",
      "Validation loss at 2500 steps: 0.0010152747854590416\n",
      "Validation loss at 2600 steps: 0.0012536760186776519\n",
      "Validation loss at 2700 steps: 0.0010937022743746638\n",
      "Validation loss at 2800 steps: 0.0017808291595429182\n",
      "Validation loss at 2900 steps: 0.0016287700273096561\n",
      "Validation loss at 3000 steps: 0.0019651043694466352\n",
      "Validation loss at 3100 steps: 0.0018641274655237794\n",
      "Validation loss at 3200 steps: 0.0019927858375012875\n",
      "Validation loss at 3300 steps: 0.0021775218192487955\n",
      "Validation loss at 3400 steps: 0.0021683508530259132\n",
      "Validation loss at 3500 steps: 0.0020908676087856293\n",
      "Validation loss at 3600 steps: 0.0022276167292147875\n",
      "Validation loss at 3700 steps: 0.002547068055719137\n",
      "Validation loss at 3800 steps: 0.0025771271903067827\n",
      "Validation loss at 3900 steps: 0.0026347474195063114\n",
      "Validation loss at 4000 steps: 0.0025368807837367058\n",
      "Validation loss at 4100 steps: 0.0029751998372375965\n",
      "Validation loss at 4200 steps: 0.003032163716852665\n",
      "Validation loss at 4300 steps: 0.0028553951997309923\n",
      "Validation loss at 4400 steps: 0.0031859942246228456\n",
      "Validation loss at 4500 steps: 0.0027847588062286377\n",
      "Validation loss at 4600 steps: 0.0027876223903149366\n",
      "Validation loss at 4700 steps: 0.0030831461772322655\n",
      "Validation loss at 4800 steps: 0.0033407642040401697\n",
      "Validation loss at 4900 steps: 0.003273238427937031\n",
      "Predicted latency: tensor([0.6375, 0.9558, 0.8003])\n",
      "Real latency: (0.6087195260822773, 0.9900684751570225, 0.7556301683187485)\n",
      "Loss: 0.0013326811604201794\n",
      "RMSE: 7.301186561584473\n",
      "MAPD: 0.04700051620602608\n",
      "Latency predictor training finished\n",
      "Example config: {'encoder': {'encoder_embed_dim': 512, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [3072, 3072, 3072, 3072, 3072, 3072], 'encoder_self_attention_heads': [8, 8, 8, 8, 8, 4]}, 'decoder': {'decoder_embed_dim': 512, 'decoder_layer_num': 5, 'decoder_ffn_embed_dim': [2048, 3072, 3072, 3072, 1024], 'decoder_self_attention_heads': [4, 8, 8, 4, 4], 'decoder_ende_attention_heads': [4, 8, 8, 4, 4], 'decoder_arbitrary_ende_attn': [-1, 1, 1, 1, 1]}}\n",
      "Example latency: 194.02683973312378\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p baseline/latpred # stores latency predictor checkpoint\n",
    "!python latency_predictor.py --configs=configs/wmt14.en-de/latency_predictor/gpu_titanxp.yml --feature-norm 640 6 2048 6 640 6 2048 6 6 2 --feature-dim 10 --lat-dataset-path baseline/genlatdata/wmt14.en-de_gpu_titanxp.csv --ckpt-path baseline/latpred/wmt14.en-de_gpu_titanxp.pt --bsz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_rDgZ_34pr1"
   },
   "source": [
    "The latency predictor can be accessed at `baseline/latpred/wmt14.en-de_gpu_titanxp.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7SsMHSn5jNm"
   },
   "source": [
    "### 4. Evolutionary search\n",
    "\n",
    "Now, we have a latency and performance predictor to quickly get latency and performance of a candidate architecture. We can perform evolutionary search that also takes latency constraint (less than 200 milliseconds) to identify efficient architecture that maximizes the BLEU score, while satisfying the constraint.\n",
    "\n",
    "Run the following command to start the search (remove `--evo-iter 1 --parent-size 2 --mutation-size 2 --crossover-size 2 --population-size 6` for full search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RiMFMII_4u7H",
    "outputId": "7a72da52-36f5-4ed2-81bc-6ce47e3c77ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, ckpt_path='baseline/latpred/wmt14.en-de_gpu_titanxp.pt', clip_norm=0.0, configs='configs/wmt14.en-de/supertransformer/space0.yml', cpu=False, criterion='label_smoothed_cross_entropy', crossover_size=2, curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=None, decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[640, 512], decoder_embed_dim=640, decoder_embed_dim_subtransformer=None, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=None, decoder_ende_attention_heads_choice=[8, 4], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=None, decoder_ffn_embed_dim_choice=[3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[6, 5, 4, 3, 2, 1], decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=None, decoder_self_attention_heads_choice=[8, 4], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[640, 512], encoder_embed_dim=640, encoder_embed_dim_subtransformer=None, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=None, encoder_ffn_embed_dim_choice=[3072, 2048, 1024], encoder_layer_num_choice=[6], encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=None, encoder_self_attention_heads_choice=[8, 4], evo_configs='configs/wmt14.en-de/evo_search/wmt14ende_titanxp.yml', evo_iter=1, feature_norm=[640.0, 6.0, 2048.0, 6.0, 640.0, 6.0, 2048.0, 6.0, 6.0, 2.0], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, lat_norm=200.0, latency_constraint=200.0, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=40000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', mutation_prob=0.3, mutation_size=2, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', parent_size=2, path=None, pdb=False, population_size=6, prefix_size=0, print_alignment=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='baseline/supernet/checkpoint_best.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='checkpoints/wmt14.en-de/supertransformer/space0', save_interval=10, save_interval_updates=0, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='checkpoints/wmt14.en-de/supertransformer/space0/tensorboard', threshold_loss_scale=None, train_subset='train', unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_cnt_max=1000000000.0, valid_subset='valid', validate_interval=10, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=10000, weight_decay=0.0, write_config_path='baseline/evosearch/wmt14.en-de_gpu_titanxp.yml')\n",
      "| [en] dictionary: 32768 types\n",
      "| [de] dictionary: 32768 types\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
      "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
      "| Fallback to xavier initializer\n",
      "TransformerSuperModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| loaded checkpoint baseline/supernet/checkpoint_best.pt (epoch 1 @ 5 updates)\n",
      "| loading train data for epoch 1\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
      "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
      "| Start Iteration 0:\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| epoch 001 | valid on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| Iteration 0, Lowest loss: 15.630878277731018\n",
      "| Config for lowest loss model: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [2048, 1024, 1024, 3072, 2048, 1024], 'encoder_self_attention_heads': [8, 4, 8, 8, 4, 4]}, 'decoder': {'decoder_embed_dim': 640, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [3072, 3072, 1024, 3072, 1024, 1024], 'decoder_self_attention_heads': [8, 8, 4, 8, 4, 4], 'decoder_ende_attention_heads': [4, 8, 4, 4, 4, 4], 'decoder_arbitrary_ende_attn': [1, 2, -1, -1, 2, 1]}}\n",
      "| Predicted latency for lowest loss model: 157.26901292800903\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p baseline/evosearch # to store best architecture config\n",
    "!CUDA_VISIBLE_DEVICES=0 python evo_search.py --configs=configs/wmt14.en-de/supertransformer/space0.yml --evo-configs=configs/wmt14.en-de/evo_search/wmt14ende_titanxp.yml --restore-file baseline/supernet/checkpoint_best.pt --ckpt-path baseline/latpred/wmt14.en-de_gpu_titanxp.pt --feature-norm 640 6 2048 6 640 6 2048 6 6 2 --write-config-path baseline/evosearch/wmt14.en-de_gpu_titanxp.yml --evo-iter 1 --parent-size 2 --mutation-size 2 --crossover-size 2 --population-size 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjY5_1fA7rFR"
   },
   "source": [
    "The config for efficient architecture can be found at: `baseline/evosearch/wmt14.en-de_gpu_titanxp.yml`.\n",
    "\n",
    "Let us take a look at this config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMEyd5qk7xg9",
    "outputId": "e99fb69a-e5bf-4754-db7a-7a90dbf782b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder-embed-dim-subtransformer: 640\n",
      "decoder-embed-dim-subtransformer: 640\n",
      "\n",
      "encoder-ffn-embed-dim-all-subtransformer: [2048, 1024, 1024, 3072, 2048, 1024]\n",
      "decoder-ffn-embed-dim-all-subtransformer: [3072, 3072, 1024, 3072]\n",
      "\n",
      "encoder-layer-num-subtransformer: 6\n",
      "decoder-layer-num-subtransformer: 4\n",
      "\n",
      "encoder-self-attention-heads-all-subtransformer: [8, 4, 8, 8, 4, 4]\n",
      "decoder-self-attention-heads-all-subtransformer: [8, 8, 4, 8]\n",
      "decoder-ende-attention-heads-all-subtransformer: [4, 8, 4, 4]\n",
      "\n",
      "decoder-arbitrary-ende-attn-all-subtransformer: [1, 2, -1, -1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat baseline/evosearch/wmt14.en-de_gpu_titanxp.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKfIpQbS8XP1"
   },
   "source": [
    "### 5. Train efficient architecture from scratch\n",
    "\n",
    "Now, we have the efficient architecture. All that is left is to train the architecture from scratch (random initialization) to convergence. The trained architecture should be ideal for deployment in the target hardware.\n",
    "\n",
    "Run the following command to train the efficient model (remove `---max-update 5 --save-interval-updates 5` for full training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92sizj7x8vWj",
    "outputId": "7721959a-7037-4e3c-c314-ef950f802d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Configs: Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformersuper_wmt_en_de', attention_dropout=0.1, beam=5, best_checkpoint_metric='loss', bucket_cap_mb=25, clip_norm=0.0, configs='baseline/evosearch/wmt14.en-de_gpu_titanxp.yml', cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data/binary/wmt16_en_de', dataset_impl=None, ddp_backend='no_c10d', decoder_arbitrary_ende_attn_all_subtransformer=[1, 2, -1, -1], decoder_arbitrary_ende_attn_choice=[-1, 1, 2], decoder_attention_heads=8, decoder_embed_choice=[512, 256, 128], decoder_embed_dim=640, decoder_embed_dim_subtransformer=640, decoder_embed_path=None, decoder_ende_attention_heads_all_subtransformer=[4, 8, 4, 4], decoder_ende_attention_heads_choice=[16, 8, 4, 2, 1], decoder_ffn_embed_dim=3072, decoder_ffn_embed_dim_all_subtransformer=[3072, 3072, 1024, 3072], decoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], decoder_input_dim=640, decoder_layer_num_choice=[7, 6, 5, 4, 3, 2], decoder_layer_num_subtransformer=4, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=640, decoder_self_attention_heads_all_subtransformer=[8, 8, 4, 8], decoder_self_attention_heads_choice=[16, 8, 4, 2, 1], device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, diverse_beam_groups=-1, diverse_beam_strength=0.5, dropout=0.3, encoder_attention_heads=8, encoder_embed_choice=[512, 256, 128], encoder_embed_dim=640, encoder_embed_dim_subtransformer=640, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_ffn_embed_dim_all_subtransformer=[2048, 1024, 1024, 3072, 2048, 1024], encoder_ffn_embed_dim_choice=[4096, 3072, 2048, 1024], encoder_layer_num_choice=[7, 6, 5, 4, 3, 2], encoder_layer_num_subtransformer=6, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_self_attention_heads_all_subtransformer=[8, 4, 8, 8, 4, 4], encoder_self_attention_heads_choice=[16, 8, 4, 2, 1], find_unused_parameters=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, get_attn=False, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, latcpu=False, latgpu=False, latiter=300, latsilent=False, lazy_load=False, left_pad_source='True', left_pad_target='False', lenpen=1, log_format=None, log_interval=1000, lr=[1e-07], lr_period_updates=-1, lr_scheduler='cosine', lr_shrink=1.0, match_source_len=False, max_epoch=0, max_len_a=0, max_len_b=200, max_lr=0.001, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=5, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, min_lr=-1, model_overrides='{}', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_repeat_ngram_size=0, no_save=False, no_save_optimizer_state=False, no_token_positional_embeddings=False, num_workers=10, optimizer='adam', optimizer_overrides='{}', path=None, pdb=False, prefix_size=0, print_alignment=False, profile_flops=False, profile_latency=False, qkv_dim=512, quiet=False, raw_text=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', results_path=None, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, save_dir='baseline/effnet', save_interval=10, save_interval_updates=5, score_reference=False, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, sub_configs='configs/wmt14.en-de/subtransformer/common.yml', t_mult=1, target_lang=None, task='translation', tbmf_wrapper=False, temperature=1.0, tensorboard_logdir='baseline/effnet/tensorboard', threshold_loss_scale=None, train_subset='train', train_subtransformer=True, unkpen=0, unnormalized=False, update_freq=[16], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=10, validate_subtransformer=False, vocab_original_scaling=False, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0)\n",
      "| [en] dictionary: 32768 types\n",
      "| [de] dictionary: 32768 types\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.en\n",
      "| loaded 3000 examples from: data/binary/wmt16_en_de/valid.en-de.de\n",
      "| data/binary/wmt16_en_de valid en-de 3000 examples\n",
      "| Fallback to xavier initializer\n",
      "| Model: transformersuper_wmt_en_de \n",
      "| Criterion: LabelSmoothedCrossEntropyCriterion\n",
      " \n",
      "\n",
      "\t\tWARNING!!! Training one single SubTransformer\n",
      "\n",
      "\n",
      "| SubTransformer Arch: {'encoder': {'encoder_embed_dim': 640, 'encoder_layer_num': 6, 'encoder_ffn_embed_dim': [2048, 1024, 1024, 3072, 2048, 1024], 'encoder_self_attention_heads': [8, 4, 8, 8, 4, 4]}, 'decoder': {'decoder_embed_dim': 640, 'decoder_layer_num': 4, 'decoder_ffn_embed_dim': [3072, 3072, 1024, 3072], 'decoder_self_attention_heads': [8, 8, 4, 8], 'decoder_ende_attention_heads': [4, 8, 4, 4], 'decoder_arbitrary_ende_attn': [1, 2, -1, -1]}} \n",
      "\n",
      "| SubTransformer size (without embedding weights): 44652544\n",
      "| Embedding layer size: 20971520 \n",
      "\n",
      "| Training on 1 GPUs\n",
      "| Max tokens per GPU = 4096 and max sentences per GPU = None \n",
      "\n",
      "| no existing checkpoint found baseline/effnet/checkpoint_last.pt\n",
      "| loading train data for epoch 0\n",
      "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.en\n",
      "| loaded 4500966 examples from: data/binary/wmt16_en_de/train.en-de.de\n",
      "| data/binary/wmt16_en_de train en-de 4500966 examples\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| epoch 001:   0% 0/2376 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| epoch 001:   0% 1/2376 [00:04<2:42:46,  4.11s/it]| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| epoch 001:   0% 2/2376 [00:06<1:57:04,  2.96s/it]| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 001:   0% 3/2376 [00:08<1:41:28,  2.57s/it]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 001:   0% 4/2376 [00:10<1:36:55,  2.45s/it]/content/hardware-aware-transformers/fairseq/optim/adam.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "| epoch 001:   0% 8/2376 [00:19<1:29:13,  2.26s/it, loss=15.642, wps=3410, num_updates=4, lr=1.0999e-06] \n",
      "| epoch 001 | validate on 'valid' subset:   0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:   3% 1/30 [00:00<00:18,  1.58it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  13% 4/30 [00:00<00:04,  6.38it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  23% 7/30 [00:00<00:02, 10.66it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  33% 10/30 [00:01<00:01, 13.95it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  43% 13/30 [00:01<00:01, 16.71it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  53% 16/30 [00:01<00:00, 18.66it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  63% 19/30 [00:01<00:00, 20.41it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  73% 22/30 [00:01<00:00, 21.48it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  83% 25/30 [00:01<00:00, 21.94it/s]\u001b[A\n",
      "| epoch 001 | validate on 'valid' subset:  93% 28/30 [00:01<00:00, 22.05it/s]\u001b[A\n",
      "                                                                             \u001b[A| epoch 001 | validate on 'valid' subset | loss 15.616 | nll_loss 15.602 | ppl 49747.46 | num_updates 5 | subtransformer_loss 15.616 | subtransformer_nll_loss 15.602\n",
      "| saved checkpoint baseline/effnet/checkpoint_1_5.pt (epoch 1 @ 5 updates) (writing took 11.896681547164917 seconds)\n",
      "| epoch 001 | loss 15.640 | nll_loss 15.630 | ppl 50722.12 | wps 3448 | ups 0 | wpb 59097.200 | bsz 1984.000 | num_updates 5 | lr 1.34987e-06 | gnorm 4.237 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 86 | train_wall 20\n",
      "| Done training in 66.7 seconds\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p baseline/effnet # stores the checkpoint for efficient model\n",
    "!python -B train.py --configs=baseline/evosearch/wmt14.en-de_gpu_titanxp.yml --save-dir baseline/effnet --sub-configs=configs/wmt14.en-de/subtransformer/common.yml --no-epoch-checkpoints --max-update 5 --save-interval-updates 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB1gRtZs-WdK"
   },
   "source": [
    "The checkpoint for the efficient model can be accessed at `baseline/effnet/checkpoint_best.pt`.\n",
    "\n",
    "## 5.1 Get performance of the efficient architecture\n",
    "\n",
    "Change the line 81 in `fairseq/search.py` from `torch.div(self.indices_buf, vocab_size, out=self.beams_buf)` to `self.beams_buf = torch.div(self.indices_buf, vocab_size).type_as(self.beams_buf)`. Otherwise, you will get the error `RuntimeError: result type Float can't be cast to the desired output type Long`.\n",
    "\n",
    "Get the BLEU score on the validation set by running the following command (expect the results to be 0 as we only did a trial run of HAT pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUyFULVv-lwQ",
    "outputId": "08b1272e-1db5-40c0-87cb-ea136bcfbdfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerSuperModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "baseline/effnet/checkpoint_best.pt\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      " 12% 3/24 [00:28<03:13,  9.19s/it, wps=2730]/content/hardware-aware-transformers/fairseq/sequence_generator.py:394: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=eos_bbsz_idx,\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:402: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [512]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=eos_scores,\n",
      " 25% 6/24 [00:55<02:46,  9.24s/it, wps=2753]/content/hardware-aware-transformers/fairseq/sequence_generator.py:451: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=active_mask,\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:459: UserWarning: An output with one or more elements was resized since it had shape [128, 4], which does not match the required output shape [127, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(new_blacklist, active_hypos)\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:469: UserWarning: An output with one or more elements was resized since it had shape [128, 4], which does not match the required output shape [127, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=active_bbsz_idx,\n",
      "/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:402: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [508]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=eos_scores,\n",
      " 29% 7/24 [01:05<02:38,  9.31s/it, wps=2747]/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [128, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      " 96% 23/24 [04:09<00:13, 13.21s/it, wps=2369]/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [56, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      "Evaluate Normal BLEU score!\n",
      "Namespace(ignore_case=False, order=4, ref='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_valid_gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_valid_gen.out.sys')\n",
      "BLEU4 = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=26.483, syslen=1701492, reflen=64248)\n"
     ]
    }
   ],
   "source": [
    "!bash configs/wmt14.en-de/test.sh baseline/effnet/checkpoint_best.pt baseline/evosearch/wmt14.en-de_gpu_titanxp.yml normal 0 valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKgJIddPDKm2"
   },
   "source": [
    "Get the BLEU score on the test set by running the following command (expect the results to be 0 as we only did a trial run of HAT pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkLFdJDVDNRx",
    "outputId": "d32dfcdb-0a68-4f5b-aa20-bbbf0d7cc7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerSuperModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): EmbeddingSuper(32768, 640, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:4\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttentionSuper\tnum_heads:8\t qkv_dim:512\n",
      "          (out_proj): LinearSuper(in_features=512, out_features=640, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): LinearSuper(in_features=640, out_features=3072, bias=True)\n",
      "        (fc2): LinearSuper(in_features=3072, out_features=640, bias=True)\n",
      "        (final_layer_norm): LayerNormSuper((640,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "baseline/effnet/checkpoint_best.pt\n",
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "  8% 2/24 [00:19<03:34,  9.74s/it, wps=2588]/content/hardware-aware-transformers/fairseq/sequence_generator.py:451: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=active_mask,\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:459: UserWarning: An output with one or more elements was resized since it had shape [128, 4], which does not match the required output shape [127, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(new_blacklist, active_hypos)\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:469: UserWarning: An output with one or more elements was resized since it had shape [128, 4], which does not match the required output shape [127, 4]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=active_bbsz_idx,\n",
      "/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [127, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:394: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [0]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=eos_bbsz_idx,\n",
      "/content/hardware-aware-transformers/fairseq/sequence_generator.py:402: UserWarning: An output with one or more elements was resized since it had shape [1], which does not match the required output shape [508]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=eos_scores,\n",
      " 12% 3/24 [00:28<03:16,  9.37s/it, wps=2672]/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [127, 8], which does not match the required output shape [128, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      " 96% 23/24 [04:13<00:13, 13.12s/it, wps=2337]/content/hardware-aware-transformers/fairseq/search.py:79: UserWarning: An output with one or more elements was resized since it had shape [128, 8], which does not match the required output shape [59, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  out=(self.scores_buf, self.indices_buf),\n",
      "Evaluate Normal BLEU score!\n",
      "Namespace(ignore_case=False, order=4, ref='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_test_gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='baseline/effnet/exp/wmt14.en-de_gpu_titanxp.yml_test_gen.out.sys')\n",
      "BLEU4 = 0.00, 0.0/0.0/0.0/0.0 (BP=1.000, ratio=26.532, syslen=1711222, reflen=64496)\n"
     ]
    }
   ],
   "source": [
    "!bash configs/wmt14.en-de/test.sh baseline/effnet/checkpoint_best.pt baseline/evosearch/wmt14.en-de_gpu_titanxp.yml normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA0eRJl8DZr2"
   },
   "source": [
    "If you want to get the performance of efficient architecture by extracting the weights from supernet (instead of using the standalone training done in Step 5), change the input from `baseline/effnet/checkpoint_best.pt` to `baseline/supernet/checkpoint_best.pt`.\n",
    "\n",
    "That's all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TVbj2bLDLu1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0opSWSHDUYrn"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
