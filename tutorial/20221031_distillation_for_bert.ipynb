{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation for Efficient BERT Models\n",
        "\n",
        "In this tutorial, we will go over the high-level theory and implementation details of various distillation techniques for building efficient task-specific BERT models. \n",
        "\n",
        "This notebook was created by [Ganesh Jawahar](ganeshjwhr@gmail.com). Contact me for any questions or suggestions."
      ],
      "metadata": {
        "id": "yC9ZN5QyaFPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "- [PyTorch](https://pytorch.org/)\n",
        "- [Transformers](https://arxiv.org/abs/1706.03762)\n",
        "- [BERT](https://arxiv.org/abs/1810.04805)"
      ],
      "metadata": {
        "id": "KeRFfudYcWPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Setting (General)\n",
        "The goal of distillation is to transfer knowledge from a large model (also known as teacher model) to a small model (also known as student model). Teacher model is typically large (in terms of model size) and hence needs large amount of memory to be loaded into device memory and slow to make predictions. On the other hand, student model is typically small and hence needs less memory to be loaded into device memory and relatively fast to make predictions. Usually, the student model is trained to reduce the distance between one or more of these:\n",
        "1. Student and teacher prediction probabilities (**logits distillation**) \n",
        "2. Student and teacher hidden layer outputs (**hidden layer distillation**)\n",
        "3. Student and teacher token embeddings (**embedding distillation**)\n",
        "4. Student and teacher self-attention matrices (**self-attention distillation**)\n",
        "5. Student and teacher value-value matrices (**value-relation distillation**)\n",
        "6. Student prediction and gold label (**task-specific loss**) \n",
        "\n",
        "## Problem Setting (This Tutorial)\n",
        "\n",
        "In this tutorial, we will focus on distilling a student model from the pre-trained BERT model (teacher model). Some of the existing research include:\n",
        "\n",
        "1. **[DistilBERT](https://arxiv.org/abs/1910.01108)** - Triple loss: (1) Distillation over soft target probabilities of teacher, (2) Supervised training loss (MLM), and (3) Cosine embedding loss that aligns student and teacher last hidden states.\n",
        "2. **[Patient Knowledge Distillation](https://arxiv.org/abs/1908.09355)** - Triple loss: (1) PKD-Last that has task-specific loss, (2) soft-target disitillation loss, and (3) hidden states MSE loss. PKD-Last distills from last k layers, while PKD-Skip distills from every k layers.\n",
        "3. **[TinyBERT](https://arxiv.org/abs/1909.10351)** - Embedding distillation, hidden and attention distillation, logits distillation.\n",
        "4. **[MiniLM](https://arxiv.org/abs/2002.10957)** - Self-attention and value-relation distillation.\n",
        "\n",
        "![HAT block diagram](https://d3i71xaburhd42.cloudfront.net/459b34447952feebacc2f12778c539618e8a299f/5-Figure2-1.png)\n",
        "\n",
        "(Picture courtesy: [A Survey on Model Compression for Natural Language Processing\n",
        "](https://arxiv.org/abs/2202.07105))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vUvbsVKYcxS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution (This Tutorial)\n",
        "\n",
        "In this tutorial, we show the application of logits, hidden layer, and embedding to distill a 6-layer student model from 12-layer BERT-base model.\n",
        "\n",
        "Let's start with the standard implementation of fine-tuning BERT model for a classification task."
      ],
      "metadata": {
        "id": "KzgK7vhPGjOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YmfNjZqRwU1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38374154-4b3a-4184-aa92-77d180eb7434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Set seed of randomization and working device\n",
        "manual_seed = 77\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed(manual_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKliaTJfIz1d",
        "outputId": "8cfbf3a8-c61c-4e47-c0d6-1267c4a5b752"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install `transformers` library"
      ],
      "metadata": {
        "id": "v6Xli3aPJRPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG96ubDJJRT2",
        "outputId": "0e6d5333-8500-42b1-c23d-2b66e3cad24d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary classes: `BertModel`, `BertForSequenceClassification`"
      ],
      "metadata": {
        "id": "W-4nwuSzJkGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizerFast, BertForSequenceClassification"
      ],
      "metadata": {
        "id": "n2ynqX-DJbv2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data, load tokenizer\n",
        "\n",
        "In this tutorial, the downstream task we will focus on is `sociality classification` (part of [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0)). This task takes a piece of text and maps into one of the two classes: (1) the author of the text interacts with other people in the emotion situation and (2) the author of the text doesn't interact with other people. The train (`train.tsv`), validation (`dev.tsv`) and test size (`test.tsv`) consist of $8448$, $2112$ and $2112$ records respectively. "
      ],
      "metadata": {
        "id": "wPWhEeOlJ45i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for data preparation\n",
        "def data_prepare(file_path, lab2ind, tokenizer, max_len = 32, mode = 'train'):\n",
        "    '''\n",
        "    file_path: the path to input file. \n",
        "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
        "                The first row must be header of columns.\n",
        "\n",
        "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
        "                The first row must be header of column.\n",
        "\n",
        "    lab2ind: dictionary of label classes\n",
        "    tokenizer: BERT tokenizer\n",
        "    max_len: maximal length of input sequence\n",
        "    mode: train or predict\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content','label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        labels = df.label.values\n",
        "        \n",
        "        # Create sentence and label lists\n",
        "        labels = [lab2ind[i] for i in labels] \n",
        "        print(\"Label is \", labels[0])\n",
        "        \n",
        "        # Convert data into torch tensors\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        # create placeholder\n",
        "        labels = []\n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    # Create sentence and label lists\n",
        "    content = df.content.values\n",
        "\n",
        "    #### REF START ####\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "    \n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "    #### REF END ####\n",
        "\n",
        "    return inputs, labels, masks\n"
      ],
      "metadata": {
        "id": "4U6IdiflJy0y"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the model path\n",
        "model_path = \"bert-base-uncased\"\n",
        "\n",
        "# label dictionary\n",
        "lab2ind = {'no': 0, 'yes': 1}\n",
        "\n",
        "# tokenizer for pre-trained BERT model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# preprocess the data\n",
        "train_inputs, train_labels, train_masks = data_prepare(\"./drive/My Drive/Colab Notebooks/happy_db/train.tsv\", lab2ind,tokenizer)\n",
        "validation_inputs, validation_labels, validation_masks = data_prepare(\"./drive/My Drive/Colab Notebooks/happy_db/dev.tsv\", lab2ind,tokenizer)\n",
        "\n",
        "# set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# take training samples in random order in each epoch. \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, \n",
        "                              sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "# Read validation set sequentially.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, \n",
        "                                   sampler = SequentialSampler(validation_data), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7VLq-_GLk7O",
        "outputId": "d2b04bd0-f126-440f-a346-302247d4dce8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size  (8448, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'it', 'was', 'my', 'birthday', ',', 'and', 'my', 'wife', 'and', 'daughter', 'surprised', 'me', 'with', 'some', 'surprise', 'guests', 'and', 'a', 'small', 'party', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 2009, 2001, 2026, 5798, 1010, 1998, 2026, 2564, 1998, 2684, 4527, 2033, 2007, 2070, 4474, 6368, 1998, 1037, 2235, 2283, 1012, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [ 101 2009 2001 2026 5798 1010 1998 2026 2564 1998 2684 4527 2033 2007\n",
            " 2070 4474 6368 1998 1037 2235 2283 1012  102    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "Data size  (1056, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'my', 'baby', 'took', 'a', '1', '.', '5', 'hour', 'nap', 'instead', 'of', 'a', '20', '##min', '##ute', 'nap', 'and', 'i', 'was', 'able', 'to', 'get', 'some', 'things', 'done', '!', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 2026, 3336, 2165, 1037, 1015, 1012, 1019, 3178, 18996, 2612, 1997, 1037, 2322, 10020, 10421, 18996, 1998, 1045, 2001, 2583, 2000, 2131, 2070, 2477, 2589, 999, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  2026  3336  2165  1037  1015  1012  1019  3178 18996  2612  1997\n",
            "  1037  2322 10020 10421 18996  1998  1045  2001  2583  2000  2131  2070\n",
            "  2477  2589   999   102     0     0     0     0     0     0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load BERT teacher model, criterion and finetuning settings\n",
        "\n",
        "Let us load the BERT teacher model with the pre-trained weights."
      ],
      "metadata": {
        "id": "osIszFJ4UALE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bert_cls(nn.Module):\n",
        "\n",
        "    def __init__(self, lab2ind, model_path, hidden_size):\n",
        "        super(Bert_cls, self).__init__()\n",
        "        self.model_path = model_path\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n",
        "        \n",
        "        self.label_num = len(lab2ind)\n",
        "        \n",
        "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "\n",
        "    def forward(self, bert_ids, bert_mask):\n",
        "        outputs = self.bert_model(input_ids=bert_ids, attention_mask = bert_mask)\n",
        "        pooler_output = outputs['pooler_output']\n",
        "        attentions = outputs['attentions']\n",
        "        \n",
        "        x = self.dense(pooler_output)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        fc_output = self.fc(x)\n",
        "\n",
        "        return fc_output, outputs\n",
        "\n",
        "bert_teacher_model = Bert_cls(lab2ind, model_path, 768).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otj7S5HFUNSy",
        "outputId": "2db2c232-b10b-43e9-bb8a-32d45ff7c876"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print structure of the teacher model (12 layers, 768 hidden, 3072 FFN. Inter.):"
      ],
      "metadata": {
        "id": "sghcQ8ItVBFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_teacher_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq7JQ9rGUlc1",
        "outputId": "a42b7783-0f14-4d08-ce23-f181df658473"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert_cls(\n",
            "  (bert_model): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us set the fine-tuning settings:"
      ],
      "metadata": {
        "id": "DTjrEKEQVe9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuning hyper-parameters:\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "epochs = 3\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "# create the optimizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "optimizer = AdamW(bert_teacher_model.parameters(), lr=lr, correct_bias=False)\n",
        "\n",
        "# set the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRVnxuuxVN-n",
        "outputId": "fbb4b095-7712-49cb-c54e-430e889eb71c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us set the task specific criterion:"
      ],
      "metadata": {
        "id": "e8xljWkPWG0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the criterion \n",
        "task_specific_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "-JvCFuHOWG5O"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the model training and evaluation functions\n",
        "\n",
        "\n",
        "Let us create the training logic for single epoch:\n"
      ],
      "metadata": {
        "id": "Y1wwDAXfWnaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        outputs,_ = model(input_ids, input_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "EaZHu5pkWMyJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create the evaluation logic for a single evaluation:"
      ],
      "metadata": {
        "id": "PNQTuMEOXHtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            input_ids, input_mask, labels = batch\n",
        "\n",
        "            outputs,_ = model(input_ids, input_mask)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # delete used variables to free GPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "metadata": {
        "id": "26usjslLXF0v"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune the BERT teacher model on the classification task"
      ],
      "metadata": {
        "id": "n2lfksM4XYNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "bert_teacher_checkpoint_dir = \"./drive/My Drive/Colab Notebooks/teacher_ckpt\"\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(bert_teacher_model, train_dataloader, optimizer, scheduler, task_specific_criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_teacher_model, validation_dataloader, task_specific_criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "      'epoch': epoch,\n",
        "      'state_dict': bert_teacher_model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(state, bert_teacher_checkpoint_dir + \"/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))\n",
        "    \n",
        "print(\"Fine-tuning completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UnEkC1xXfCf",
        "outputId": "83a6b1c8-0f23-4283-ed63-b02b84a3ea74"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  33%|███▎      | 1/3 [01:01<02:03, 61.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 0.2758, Validation Loss: 0.1973, Validation Accuracy: 0.9318, Validation F1: 0.9313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [02:03<01:01, 61.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.1435, Validation Loss: 0.2111, Validation Accuracy: 0.9394, Validation F1: 0.9390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [03:05<00:00, 61.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.0906, Validation Loss: 0.2283, Validation Accuracy: 0.9403, Validation F1: 0.9399\n",
            "Fine-tuning completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Student model initialization\n",
        "\n",
        "Usually, the student BERT model is initialized with every k layers of teacher BERT model. In this tutorial, we will choose k as 3, that is, student BERT model is 4 layers (mapped to layer 2, 5, 8, 11 of teacher).\n",
        "\n",
        "Let us try to create this student model from the teacher model."
      ],
      "metadata": {
        "id": "wdZBTebTex5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_student_model(teacher_model, student_to_teacher_layer):\n",
        "  # adapted from https://github.com/huggingface/transformers/issues/2483\n",
        "\n",
        "  oldModuleList = teacher_model.bert_model.encoder.layer\n",
        "  newModuleList = nn.ModuleList()\n",
        "\n",
        "  # copy the layers to keep\n",
        "  for student_layer_id in range(len(student_to_teacher_layer)):\n",
        "    newModuleList.append(oldModuleList[student_to_teacher_layer[student_layer_id]])\n",
        "  \n",
        "  # create a copy of the model, modify it with the new list, and return\n",
        "  import copy\n",
        "  student_model = copy.deepcopy(teacher_model)\n",
        "  student_model.bert_model.encoder.layer = newModuleList\n",
        "\n",
        "  return student_model\n",
        "\n",
        "# layer_mapping from student to teacher (and reverse)\n",
        "student_to_teacher_layer = {0:2, 1:5, 2:8, 3:11}\n",
        "teacher_to_student_layer = {2:0, 5:1, 8:2, 11:3}\n",
        "\n",
        "# creates the student model\n",
        "bert_student_model = create_student_model(bert_teacher_model, student_to_teacher_layer)\n",
        "\n",
        "# print the structure of the student model\n",
        "print(bert_student_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD5ot60eez45",
        "outputId": "1082ae41-5c7d-439d-b2f7-6407571b4755"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert_cls(\n",
            "  (bert_model): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logits distillation\n",
        "\n",
        "The logits based distillation loss can be simply defined as:\n",
        "\n",
        "$\\mathcal{L}_{pred} = $Cross-Entropy$(\\frac{\\textbf{z}^T}{t}, \\frac{\\textbf{z}^S}{t})$ \n",
        "\n",
        "where $t$ corresponds to the temperature (usually set to 1). $\\textbf{z}^T$ and $\\textbf{z}^S$ correspond to teacher model and student model prediction logits respectively. \n",
        "\n",
        "Let us augment the fine-tuning loss for the student model with the logits-based distillation loss.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ao5egGYgkQ2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_with_logitsdistill(student_model, iterator, optimizer, scheduler, criterion, teacher_model):\n",
        "    \n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "    epoch_loss = 0\n",
        "    distillation_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        student_logits, _ = student_model(input_ids, input_mask)\n",
        "\n",
        "        # standard supervised loss\n",
        "        loss = criterion(student_logits, labels)\n",
        "\n",
        "        # distillation loss\n",
        "        teacher_logits, _ = teacher_model(input_ids, input_mask)\n",
        "        # compute KL-div of student logits and teacher logits\n",
        "        # https://raw.githubusercontent.com/liuzechun/ReActNet/master/utils/KD_loss.py\n",
        "        model_output_log_prob = F.log_softmax(student_logits, dim=1)\n",
        "        model_output_log_prob = model_output_log_prob.unsqueeze(2)\n",
        "\n",
        "        real_output_soft = F.softmax(teacher_logits, dim=1)\n",
        "        real_output_soft = real_output_soft.unsqueeze(1)\n",
        "        cross_entropy_loss = -torch.bmm(real_output_soft, model_output_log_prob)\n",
        "        cross_entropy_loss = cross_entropy_loss.mean()\n",
        "        distillation_loss += cross_entropy_loss.item()\n",
        "        loss += cross_entropy_loss\n",
        "\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator), distillation_loss / len(iterator)\n",
        "\n",
        "# Train the student model\n",
        "student_checkpoint_dir = \"./drive/My Drive/Colab Notebooks/student_ckpt\"\n",
        "\n",
        "optimizer = AdamW(bert_student_model.parameters(), lr=lr, correct_bias=False)\n",
        "\n",
        "# set the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss, distill_loss = train_with_logitsdistill(bert_student_model, train_dataloader, optimizer, scheduler, task_specific_criterion, bert_teacher_model)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_student_model, validation_dataloader, task_specific_criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "      'epoch': epoch,\n",
        "      'state_dict': bert_teacher_model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(state, bert_teacher_checkpoint_dir + \"/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Train Distill Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, distill_loss, val_loss, val_acc, val_f1))\n",
        "    \n",
        "print(\"Fine-tuning completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjWDE7A0nhYv",
        "outputId": "13118ead-1b85-4ac6-8c3b-b472b1e2cb74"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Epoch:  33%|███▎      | 1/3 [01:07<02:14, 67.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 0.4314, Train Distill Loss: 0.1869, Validation Loss: 0.2402, Validation Accuracy: 0.9252, Validation F1: 0.9249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [02:15<01:07, 67.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.2125, Train Distill Loss: 0.0715, Validation Loss: 0.2381, Validation Accuracy: 0.9280, Validation F1: 0.9277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [03:23<00:00, 67.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.1442, Train Distill Loss: 0.0485, Validation Loss: 0.2789, Validation Accuracy: 0.9299, Validation F1: 0.9296\n",
            "Fine-tuning completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden and Embedding distillation\n",
        "\n",
        "The hidden state distillation loss can be simply defined as:\n",
        "\n",
        "$\\mathcal{L}_{hidn} = $Mean-Squared-Error$(\\textbf{H}^S, \\textbf{H}^T)$ \n",
        "\n",
        "where $\\textbf{H}^S$ corresponds to the student hidden states ($\\#layers \\times $hidden-dim) and $\\textbf{H}^T$ corresponds to the teacher hidden states ($\\#layers \\times $hidden-dim).\n",
        "\n",
        "The embedding distillation loss can be simply defined as:\n",
        "\n",
        "$\\mathcal{L}_{embd} = $Mean-Squared-Error$(\\textbf{e}^S, \\textbf{e}^T)$\n",
        "\n",
        "where $\\textbf{e}^S$ and $\\textbf{e}^T$ correspond to student and teacher input embeddings respectively.\n",
        "\n",
        "Let us augment the fine-tuning loss for the student model with the hidden state and embedding distillation loss.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zg8eLHQZ6QnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_with_hidembddistill(student_model, iterator, optimizer, scheduler, criterion, teacher_model):\n",
        "    \n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "    epoch_loss = 0\n",
        "    distillation_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        student_logits, student_outputs  = student_model(input_ids, input_mask)\n",
        "\n",
        "        # standard supervised loss\n",
        "        loss = criterion(student_logits, labels)\n",
        "\n",
        "        # distillation loss\n",
        "        teacher_logits, teacher_outputs = teacher_model(input_ids, input_mask)\n",
        "        full_loss = 0.0\n",
        "        # hidden states\n",
        "        for student_layer_id in student_to_teacher_layer:\n",
        "          teacher_layer_id = student_to_teacher_layer[student_layer_id]\n",
        "          non_trainable_layernorm = nn.LayerNorm(teacher_outputs[\"hidden_states\"][teacher_layer_id+1].shape[1:], elementwise_affine=False)\n",
        "          teacher_hidden, student_hidden  = teacher_outputs[\"hidden_states\"][teacher_layer_id+1], student_outputs[\"hidden_states\"][student_layer_id+1]\n",
        "          teacher_hidden = non_trainable_layernorm(teacher_hidden) \n",
        "          student_hidden = non_trainable_layernorm(student_hidden)\n",
        "          cur_fkt = nn.MSELoss()(teacher_hidden, student_hidden)\n",
        "          full_loss += cur_fkt\n",
        "        # embedding \n",
        "        teacher_layer_id = student_to_teacher_layer[0]\n",
        "        non_trainable_layernorm = nn.LayerNorm(teacher_outputs[\"hidden_states\"][0].shape[1:], elementwise_affine=False)\n",
        "        teacher_hidden, student_hidden  = teacher_outputs[\"hidden_states\"][0], student_outputs[\"hidden_states\"][0]\n",
        "        teacher_hidden = non_trainable_layernorm(teacher_hidden) \n",
        "        student_hidden = non_trainable_layernorm(student_hidden)\n",
        "        cur_fkt = nn.MSELoss()(teacher_hidden, student_hidden)\n",
        "        full_loss += cur_fkt\n",
        "        # full distillation loss\n",
        "        full_loss = full_loss / (float(len(student_to_teacher_layer))+1)\n",
        "        distillation_loss += full_loss.item()\n",
        "        loss += full_loss\n",
        "\n",
        "        # delete used variables to free GPU memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "    # free GPU memory\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return epoch_loss / len(iterator), distillation_loss / len(iterator)\n",
        "\n",
        "# Train the student model\n",
        "student_checkpoint_dir = \"./drive/My Drive/Colab Notebooks/student_ckpt\"\n",
        "\n",
        "optimizer = AdamW(bert_student_model.parameters(), lr=lr, correct_bias=False)\n",
        "\n",
        "# set the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss, distill_loss = train_with_hidembddistill(bert_student_model, train_dataloader, optimizer, scheduler, task_specific_criterion, bert_teacher_model)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_student_model, validation_dataloader, task_specific_criterion)\n",
        "    \n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "      'epoch': epoch,\n",
        "      'state_dict': bert_teacher_model.state_dict(),\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'scheduler': scheduler.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(state, bert_teacher_checkpoint_dir + \"/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Train Distill Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, distill_loss, val_loss, val_acc, val_f1))\n",
        "    \n",
        "print(\"Fine-tuning completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tdKFKSo8SY1",
        "outputId": "883a019d-6d48-46c7-f45e-87e103ba552d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Epoch:  33%|███▎      | 1/3 [01:09<02:19, 69.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 0.3386, Train Distill Loss: 0.2051, Validation Loss: 0.2722, Validation Accuracy: 0.9176, Validation F1: 0.9168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [02:19<01:09, 69.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.1771, Train Distill Loss: 0.0948, Validation Loss: 0.3416, Validation Accuracy: 0.9176, Validation F1: 0.9169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 3/3 [03:29<00:00, 69.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.1277, Train Distill Loss: 0.0800, Validation Loss: 0.3486, Validation Accuracy: 0.9186, Validation F1: 0.9180\n",
            "Fine-tuning completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it!"
      ],
      "metadata": {
        "id": "9uvRk0g89Jm8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AbrqKRhm9KmY"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}