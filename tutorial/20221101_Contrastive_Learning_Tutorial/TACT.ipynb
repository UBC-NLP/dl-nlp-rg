{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4604985d-8960-4fbc-a139-48f76c3b1006",
   "metadata": {},
   "source": [
    "## Token-level Adversarial Contrastive Training (TACT)\n",
    "\n",
    "This is a variant of CAT where instead of perturbing the word embedding matrix, we directly perturb the token representations. We consider this perturbed representation for the positive pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc87926-e404-4151-aa61-01e0aa75da5d",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"Images/ADV_1.png\"  width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21c839-8486-44b1-8eaf-cd2f8a9d9115",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2851941-c334-4f50-8c38-ef640eb6e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, sys, regex\n",
    "import torch\n",
    "#import GPUtil\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "##----------------------------------------------------\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import datasets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    # Set the random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a758c1-d2cf-4944-ab94-a044c3f748e6",
   "metadata": {},
   "source": [
    "### Function for Tokenizing Train & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6ab0a9-bd61-4d04-a047-2c3fb89b9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, lab2ind, text_col_1 = 'sentence', text_col_2 = None, label_col = 'labels'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text_col_1 = self.data[text_col_1]\n",
    "        if(text_col_2 is None):\n",
    "            self.text_col_2 = None\n",
    "        else:\n",
    "            self.text_col_2 = self.data[text_col_2]\n",
    "        self.labels = self.data[label_col]\n",
    "        self.max_len = max_len\n",
    "        self.lab2ind = lab2ind\n",
    "        \n",
    "        self.isPair = True\n",
    "        if(self.text_col_2 is None):\n",
    "            self.isPair = False\n",
    "            self.text_col_2 = self.data[text_col_1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_col_1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_1 = str(self.text_col_1[index])     \n",
    "        text_2 = str(self.text_col_2[index]) \n",
    "        \n",
    "        label = self.labels[index]\n",
    "        label = self.lab2ind[label]\n",
    "        try:\n",
    "            label = self.lab2ind[label]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if(self.isPair):\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "            [text_1, text_2],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        else:\n",
    "            inputs = self.tokenizer.batch_encode_plus(\n",
    "            [text_1],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            #return_token_type_ids=False, !!!!!!!!!\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        if(self.isPair):\n",
    "            dic = {\n",
    "            'ids': torch.tensor(inputs.input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs.attention_mask, dtype=torch.long),\n",
    "            'token': torch.tensor(inputs.token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        else:\n",
    "            dic = {\n",
    "            'ids': torch.tensor(inputs.input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(inputs.attention_mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c780e-9ea5-4cb3-913d-6e0dd6d68511",
   "metadata": {},
   "source": [
    "### Function for Encoding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918df8df-7fde-4957-a3cf-0702693ae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for data preparation\n",
    "def regular_encode(file_path, tokenizer, lab2ind, shuffle=True, num_workers = 1, batch_size=64, maxlen = 32, mode = 'train', text_col_1 = 'sentence', text_col_2 = None, label_col = 'labels'):\n",
    "    \n",
    "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
    "    delimiter = None\n",
    "    if(str(file_path).endswith('tsv')):\n",
    "        delimiter = '\\t'\n",
    "    if mode == 'train':\n",
    "        # Use pandas to load dataset\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind, text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    \n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    elif mode == 'evaluate':\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        custom_set = CustomDataset(df, tokenizer, maxlen,lab2ind, text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    else:\n",
    "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
    "\n",
    "        return\n",
    "        \n",
    "    print(\"{} Dataset: {}\".format(file_path, df.shape))\n",
    "    \n",
    "    dataset_params = {'batch_size': batch_size, 'shuffle': shuffle, 'num_workers': num_workers}\n",
    "\n",
    "    batch_data_loader = DataLoader(custom_set, **dataset_params)\n",
    "    \n",
    "    return batch_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56184e94-52a8-4fe1-8b70-f032ca97e341",
   "metadata": {},
   "source": [
    "### Supervised Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7399ad0f-2673-4ab2-9ab0-dc290d61628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class Soft_SupConLoss_CLS(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, temperature=0.07, device='cpu'):\n",
    "        super(Soft_SupConLoss_CLS, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, features, labels=None, weights=None, mask = None):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "\n",
    "        features = F.normalize(features, dim=1, p=2)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        weights = F.softmax(weights,dim=1) # logit to softmax\n",
    "\n",
    "        labels_one_hot = F.one_hot(labels, num_classes=self.num_classes).float().to(self.device)\n",
    "\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(self.device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(self.device)\n",
    "        else:\n",
    "            mask = mask.float().to(self.device)\n",
    "\n",
    "        contrast_feature = features\n",
    "        anchor_feature = contrast_feature\n",
    "\n",
    "        # compute dot product of embeddings\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T), \n",
    "            self.temperature)\n",
    "\n",
    "        # set diagonal as 0\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size).view(-1, 1).to(self.device),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        ## it produces 0 for the non-matching places and 1 for matching places and neg mask does the opposite\n",
    "        ## set diagonal as 0\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        weighted_mask = torch.matmul(weights, torch.transpose(labels_one_hot, 0, 1)).to(self.device)\n",
    "\n",
    "        weighted_mask = weighted_mask * logits_mask\n",
    "\n",
    "        # weights of postive samples\n",
    "        pos_weighted_mask = weighted_mask * mask\n",
    "\n",
    "        # compute log_prob with logsumexp\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "\n",
    "        # remove diagonal\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # wiyk * exp(hi * hk / t)\n",
    "        exp_logits = torch.exp(logits) * weighted_mask\n",
    "\n",
    "        ## log_prob = x - max(x1,..,xn) - logsumexp(x1,..,xn) the equation\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (pos_weighted_mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = -1 * mean_log_prob_pos\n",
    "        # loss = loss.view(anchor_count, batch_size).mean()\n",
    "        loss = loss.mean()\n",
    "        # print(\"loss\",loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d93785-99d5-4cbc-b6d9-bdb77fbc6bae",
   "metadata": {},
   "source": [
    "### Define Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b105cf50-d851-41bd-b292-8972a8ae7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_module(module):\n",
    "    torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    torch.nn.init.constant_(module.bias, 0)\n",
    "\n",
    "\n",
    "class Bert_CLS(nn.Module):\n",
    "    def __init__(self, lab2ind, model_path, hidden_size, loss_func, cl_dim = 300):\n",
    "        super(Bert_CLS, self).__init__()\n",
    "        self.model_path = model_path\n",
    "        self.hidden_size = hidden_size\n",
    "        self.loss_func = loss_func\n",
    "        self.cl_dim = cl_dim\n",
    "        \n",
    "        self.eps = 0.005\n",
    "\n",
    "        self.label_num = len(lab2ind.keys())\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
    "        \n",
    "        self.cl_dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.cl_fc = nn.Linear(self.hidden_size, self.cl_dim)\n",
    "        \n",
    "        initial_module(self.dense)\n",
    "        initial_module(self.fc)\n",
    "        initial_module(self.cl_dense)\n",
    "        initial_module(self.cl_fc)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids = None, labels=None, sequence_output = None):\n",
    "        is_seq = False\n",
    "        if(sequence_output is None):\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, \n",
    "                                    output_hidden_states = True, return_dict = True)\n",
    "            sequence_output = outputs['last_hidden_state']\n",
    "        else:\n",
    "            is_seq = True\n",
    "            sequence_output.requires_grad=True\n",
    "\n",
    "        \n",
    "\n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        if(is_seq):\n",
    "            loss = self.loss_func(logits, labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            cl_grad = sequence_output.grad.detach()\n",
    "            #print(cl_grad)\n",
    "            #print('Dec Grad Size {}'.format(dec_grad.size()))\n",
    "            l2_norm = torch.norm(cl_grad, dim=-1)\n",
    "\n",
    "            cl_grad /= (l2_norm.unsqueeze(-1) + 1e-12)\n",
    "\n",
    "            sequence_output = sequence_output + self.eps * cl_grad.detach()\n",
    "        \n",
    "        x = sequence_output[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.cl_dense(x)\n",
    "        x = F.relu(x)\n",
    "        cl_emb = self.cl_fc(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits, labels)\n",
    "            return [loss, logits, cl_emb, sequence_output]\n",
    "        else:\n",
    "            return [None, logits, cl_emb, sequence_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b738d36-e483-4cf6-b25c-17dd9aebe4dc",
   "metadata": {},
   "source": [
    "### Define Train Function for TACT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff6e4f-a14f-4d91-abd7-3cebb328c7af",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"Images/tact_code_0.PNG\"  width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bb234b-96a9-40c7-a7d8-a20be9bc3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def train(model_main, iterator, optimizer, scheduler, label_size, supervised_cl_weight, supervised_cls_temp, \n",
    "          soft_supervised_cl = False, weight_model = None, isPair = False):\n",
    "    \n",
    "    model_main.train()\n",
    "\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    if soft_supervised_cl:\n",
    "        print(\"ADV CL\")\n",
    "        loss_supercl_cls = losses.NTXentLoss(temperature=supervised_cls_temp)\n",
    "    else:\n",
    "        print(\"Supervised CL\")\n",
    "        loss_supercl = losses.SupConLoss(temperature=supervised_cls_temp)\n",
    "\n",
    "    for _, batch in enumerate(tqdm(iterator, desc=\"Iteration\")):\n",
    "        \n",
    "        input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "        try:\n",
    "            token_type_ids = batch['token'].to(device, dtype=torch.long)\n",
    "        except:\n",
    "            token_type_ids = None\n",
    "        labels = batch['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "        num_sent = input_ids.shape[1]\n",
    "\n",
    "        if (len(input_ids.shape) == 3):\n",
    "            input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "            attention_mask = attention_mask.view((-1, input_ids.size(-1)))\n",
    "        if (token_type_ids and (len(token_type_ids.shape) == 3) and (isPair==True)):\n",
    "            token_type_ids = token_type_ids.view((-1, input_ids.size(-1)))\n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            supervised_labels = labels\n",
    "            if((len(supervised_labels.shape) == 1) and (isPair==False)):\n",
    "                supervised_labels = supervised_labels.unsqueeze(1)\n",
    "                supervised_labels = torch.cat([supervised_labels] * num_sent, 1).view(-1)  # size (bs * num_sent)\n",
    "\n",
    "        outputs = model_main(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = supervised_labels)\n",
    "        loss, logits, cl_emb = outputs[:3]\n",
    "        sequence_output = outputs[3]\n",
    "\n",
    "        loss = loss * (1.0 - supervised_cl_weight)\n",
    "\n",
    "        weight_model = deepcopy(model_main)\n",
    "        sequence_output_clone = sequence_output.detach()\n",
    "        sequence_output_clone.requires_grad = True\n",
    "\n",
    "        outputs = weight_model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = supervised_labels,\n",
    "                              sequence_output=sequence_output_clone)\n",
    "\n",
    "        loss_weight, logits_adv, cl_emb_adv = outputs[:3]\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        cl_emb_new = torch.cat([cl_emb,cl_emb_adv],dim=1).view(-1,cl_emb.size(-1))\n",
    "        cont_labels = torch.arange(batch_size).unsqueeze(1)\n",
    "        cont_labels = torch.cat([cont_labels, cont_labels], dim=1)\n",
    "        cont_labels = cont_labels.view(-1)\n",
    "\n",
    "\n",
    "        if supervised_cl_weight > 0.0:\n",
    "            if soft_supervised_cl:\n",
    "                loss += loss_supercl_cls(cl_emb_new, cont_labels) * supervised_cl_weight\n",
    "            else:\n",
    "                loss += loss_supercl(cls_embedding, supervised_labels) * supervised_cl_weight\n",
    "\n",
    "\n",
    "        if torch.cuda.device_count() == 1:\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.cpu().item()\n",
    "\n",
    "        elif torch.cuda.device_count() > 1:\n",
    "            loss.mean().backward()\n",
    "            epoch_loss += loss.mean().cpu().item()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.cpu().item()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model_main.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        model_main.zero_grad()\n",
    "\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e32e7-2dd7-4bb9-aaab-5469e35febda",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd3cef1-d3c1-4948-85a7-fa81c3184b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, metric, is_regression = False, isPair = False):\n",
    "    AvgRec=0.00\n",
    "    Fpn=0.00\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator, 0):\n",
    "        # Add batch to GPU\n",
    "            input_ids = batch['ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = batch['mask'].to(device, dtype = torch.long)\n",
    "            try:\n",
    "                token_type_ids = batch['token'].to(device, dtype=torch.long)\n",
    "            except:\n",
    "                token_type_ids = None\n",
    "            labels = batch['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            if (len(input_ids.shape) == 3):\n",
    "                input_ids = input_ids.view((-1, input_ids.size(-1)))\n",
    "                attention_mask = attention_mask.view((-1, input_ids.size(-1)))\n",
    "            if (token_type_ids and (len(token_type_ids.shape) == 3) and (isPair==True)):\n",
    "                token_type_ids = token_type_ids.view((-1, input_ids.size(-1)))\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids, labels = labels)\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # delete used variables to free GPU memory\n",
    "            del batch, input_ids, attention_mask, token_type_ids\n",
    "\n",
    "            if torch.cuda.device_count() == 1:\n",
    "                epoch_loss += loss.cpu().item()\n",
    "            else:\n",
    "                epoch_loss += loss.sum().cpu().item()\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(logits.cpu() if is_regression else predicted)\n",
    "            all_label.extend(labels.cpu())\n",
    "\n",
    "   \n",
    "    if(is_regression):\n",
    "        result =  {\"mse\": (np.array((all_pred - all_label) ** 2)).mean().item()}\n",
    "    else:\n",
    "        result = metric(predictions=all_pred, references=all_label)\n",
    "    return epoch_loss/len(iterator), result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8185fe5-f04e-4b90-9da4-792e3bfeaa40",
   "metadata": {},
   "source": [
    "### Create Optimizer and Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061cdaa2-6ff6-4969-bca3-b1a8b2504172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(total_params, num_training_steps, warmup_steps, weight_decay, learning_rate, is_constant_lr):\n",
    "    \"\"\"\n",
    "    Setup the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in total_params if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in total_params if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    if is_constant_lr == True:\n",
    "    \tlr_scheduler = get_constant_schedule(optimizer)\n",
    "    else:\n",
    "\t    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "\t        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "\t    )\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3483e199-582d-4648-b43a-08235131e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean().item()\n",
    "\n",
    "def accuracy(predictions, references):\n",
    "    acc = accuracy_score(references, predictions)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841ffd5-6af9-4667-8a0c-85b87538b9f0",
   "metadata": {},
   "source": [
    "### Finetuning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3781011-48c8-469d-a4e0-16b65a7bd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(config, seed):\n",
    "    set_seed(seed)\n",
    "    #---------------------------------------\n",
    "    print(\"[INFO] step (1) load train_test config:\")\n",
    "\n",
    "    task_name = config[\"task_name\"]\n",
    "    text_col_1 = config[\"text_column_1\"]\n",
    "    isPair = False\n",
    "    try:\n",
    "        text_col_2 = config[\"text_column_2\"]\n",
    "        if(text_col_2 is not None):\n",
    "            isPair = True #Pair Sentence True\n",
    "    except: \n",
    "        text_col_2 = None\n",
    "    label_col = config[\"label_column\"]\n",
    "\n",
    "    train_file = os.path.join(config[\"data_dir\"], config[\"train_file\"])\n",
    "\n",
    "\n",
    "    try:\n",
    "        is_constant_lr = config[\"is_constant_lr\"]\n",
    "    except: \n",
    "        is_constant_lr = False\n",
    "\n",
    "    print(\"[INFO] USE Supervised Contrastive Learing:\")\n",
    "    supervised_cl = config[\"supervised_cl\"]\n",
    "    supervised_cl_weight = config[\"supervised_cl_weight\"]\n",
    "    supervised_cls_temp = config[\"supervised_cls_temp\"]\n",
    "    soft_supervised_cl = config[\"soft_supervised_cl\"] # true or false\n",
    "\n",
    "\n",
    "    if supervised_cl_weight == 0.0:\n",
    "        weight_model_flag = False\n",
    "        soft_supervised_cl = False\n",
    "\n",
    "    if soft_supervised_cl == False:\n",
    "        weight_model_flag = False\n",
    "\n",
    "    \n",
    "        \n",
    "    dev_file = os.path.join(config[\"data_dir\"], config[\"dev_file\"])\n",
    "    test_file = os.path.join(config[\"data_dir\"], config[\"test_file\"])\n",
    "\n",
    "\n",
    "    max_seq_length= int(config[\"max_seq_length\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    try: \n",
    "        early_stop = config[\"early_stop\"]\n",
    "    except:\n",
    "        early_stop = 5\n",
    "\n",
    "    try:\n",
    "        save_model = config[\"save_model\"]\n",
    "    except: \n",
    "        save_model = False\n",
    "\n",
    "\n",
    "    learning_rate = float(config[\"lr\"]) \n",
    "    model_path = config['pretrained_model_path']\n",
    "    num_epochs = config['epochs']\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------\n",
    "    print(\"[INFO] step (2) check checkpoit directory and report file:\")\n",
    "    ckpt_dir = config[\"ckpt_dir\"] + \"/\"\n",
    "    #-------------------------------------------------------\n",
    "    print(\"[INFO] step (3) load label to number dictionary:\")\n",
    "    \n",
    "    delimiter = None\n",
    "    if(str(train_file).endswith('tsv')):\n",
    "        delimiter = '\\t'\n",
    "    df = pd.read_csv(train_file, delimiter=delimiter)\n",
    "    labels = df[label_col].tolist()\n",
    "    is_regression = False\n",
    "    if(isinstance(labels[0], float)):\n",
    "        is_regression = True\n",
    "        lab2ind = {'float':0}\n",
    "    \n",
    "    unique_labels = list(set(labels))\n",
    "    lab2ind = {l:ind for ind,l in enumerate(unique_labels)}\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        num_workers = config['num_workers']\n",
    "    except:\n",
    "        num_workers = 1\n",
    "    \n",
    "    \n",
    "    print(\"[INFO] train_file\", train_file)\n",
    "    print(\"[INFO] dev_file\", dev_file)\n",
    "    print(\"[INFO] test_file\", test_file)\n",
    "    print(\"[INFO] num_epochs\", num_epochs)\n",
    "    print(\"[INFO] model_path\", model_path)\n",
    "    print(\"[INFO] max_seq_length\", max_seq_length)\n",
    "    print(\"[INFO] batch_size\", batch_size)\n",
    "    print(\"[INFO] Number of Classes\", len(lab2ind))\n",
    "    print(\"[INFO] Number of Workers\", num_workers)\n",
    "    print(\"[INFO] step (4) Use defined funtion to extract tokanize data\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if(is_regression):\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    print(\"loading Model setting\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    print(\"[INFO] step (5) Create an iterator of data with torch DataLoader.\")\n",
    "\n",
    "    train_dataloader = regular_encode(train_file, tokenizer, lab2ind, True, batch_size=batch_size, maxlen = max_seq_length, mode = \"train\",\n",
    "                                     text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "    validation_dataloader = regular_encode(dev_file, tokenizer, lab2ind, True, batch_size=batch_size, maxlen = max_seq_length, mode = \"evaluate\",\n",
    "                                     text_col_1 = text_col_1, text_col_2 = text_col_2, label_col = label_col)\n",
    "\n",
    "    model = Bert_CLS(lab2ind, model_path, 768, criterion, 300)\n",
    "\n",
    "\n",
    "    print(\"[INFO] step (6) run with parallel CPU/GPUs\")\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.device_count() == 1:\n",
    "            print(\"Run\",model_path, \"with one GPU\")\n",
    "            model = model.to(device)\n",
    "\n",
    "\n",
    "    #---------------------------------------------------\n",
    "    print(\"[INFO] step (7) set Parameters, schedules, and loss function:\")\n",
    "    global max_grad_norm\n",
    "    max_grad_norm = 1.0\n",
    "    try:\n",
    "        warmup_proportion = config[\"warmup_proportion\"]\n",
    "    except: \n",
    "        warmup_proportion = 0.06\n",
    "\n",
    "    num_training_steps\t= len(train_dataloader) * num_epochs\n",
    "    num_warmup_steps = num_training_steps * warmup_proportion\n",
    "    ### In Transformers, optimizer and schedules are instantiated like this:\n",
    "    # Note: AdamW is a class from the huggingface library\n",
    "    # the 'W' stands for 'Weight Decay\"\n",
    "    weight_decay = 0.01\n",
    "\n",
    "    total_params = list(model.named_parameters())\n",
    "\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(total_params, num_training_steps, num_warmup_steps, weight_decay, learning_rate, is_constant_lr)\n",
    "\n",
    "    metric = accuracy\n",
    "    \n",
    "    print(\"[INFO] step (8) start fine_tuning\")\n",
    "    \n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        train_loss = train(model, train_dataloader, optimizer, scheduler, len(lab2ind), supervised_cl_weight, supervised_cls_temp, soft_supervised_cl, isPair)\t  \n",
    "        eval_loss, eval_result = evaluate(model, validation_dataloader, metric, isPair=isPair)\n",
    "        print(f'Train Loss: {train_loss}')\n",
    "        print(eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bffe49b-3fdd-43d1-8e46-7037e88a68a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ef85a5f-bb89-4d16-9b6d-830a2148c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] step (1) load train_test config:\n",
      "[INFO] USE Supervised Contrastive Learing:\n",
      "[INFO] step (2) check checkpoit directory and report file:\n",
      "[INFO] step (3) load label to number dictionary:\n",
      "[INFO] train_file ./sst2_tiny.csv\n",
      "[INFO] dev_file ./sst2_tiny.csv\n",
      "[INFO] test_file ./sst2_tiny.csv\n",
      "[INFO] num_epochs 3\n",
      "[INFO] model_path bert-base-uncased\n",
      "[INFO] max_seq_length 64\n",
      "[INFO] batch_size 5\n",
      "[INFO] Number of Classes 2\n",
      "[INFO] Number of Workers 1\n",
      "[INFO] step (4) Use defined funtion to extract tokanize data\n",
      "loading Model setting\n",
      "[INFO] step (5) Create an iterator of data with torch DataLoader.\n",
      "./sst2_tiny.csv Dataset: (10, 3)\n",
      "./sst2_tiny.csv Dataset: (10, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tawkat/ENV_1/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] step (6) run with parallel CPU/GPUs\n",
      "[INFO] step (7) set Parameters, schedules, and loss function:\n",
      "[INFO] step (8) start fine_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "ADV CL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0909855365753174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  50%|█████     | 1/2 [00:04<00:04,  4.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1126958131790161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|██████████| 2/2 [00:08<00:00,  4.08s/it]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [00:10<00:21, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1018406748771667\n",
      "{'accuracy': 0.9}\n",
      "Epoch: 2\n",
      "ADV CL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9569948315620422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  50%|█████     | 1/2 [00:05<00:05,  5.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035152196884155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|██████████| 2/2 [00:08<00:00,  4.40s/it]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [00:22<00:11, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9302550256252289\n",
      "{'accuracy': 0.8}\n",
      "Epoch: 3\n",
      "ADV CL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8728577494621277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  50%|█████     | 1/2 [00:05<00:05,  5.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8933364152908325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 100%|██████████| 2/2 [00:08<00:00,  4.35s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [00:33<00:00, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8830970823764801\n",
      "{'accuracy': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {}\n",
    "\n",
    "config['task_name'] = \"Binary Sentiment Classification\"\n",
    "config['text_column_1'] = \"sentence\"\n",
    "config['text_column_2'] = None\n",
    "config['label_column'] = \"label\"\n",
    "config['data_dir'] = \"./\"\n",
    "config['train_file'] = \"sst2_tiny.csv\"\n",
    "config['dev_file'] = \"sst2_tiny.csv\"\n",
    "config['test_file'] = \"sst2_tiny.csv\"\n",
    "config['is_constant_lr'] = False\n",
    "config['supervised_cl'] = True\n",
    "config['supervised_cl_weight'] = 0.5\n",
    "config['supervised_cls_temp'] = 0.3\n",
    "config['soft_supervised_cl'] = True\n",
    "config['weight_model_flag'] = True\n",
    "config['max_seq_length'] = 64\n",
    "config['batch_size'] = 5\n",
    "config['early_stop'] = 5\n",
    "config['save_model'] = False\n",
    "config['lr'] = 0.00005\n",
    "config['pretrained_model_path'] = \"bert-base-uncased\"\n",
    "config['epochs'] = 3\n",
    "config['ckpt_dir'] = \"ckpt\"\n",
    "config['warmup_proportion'] = 0.05\n",
    "config['metric'] = \"accuracy\"\n",
    "config['num_workers'] = 1\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "fine_tuning(config, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a4e30-fdd7-467c-8bb0-b98b759fd671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
